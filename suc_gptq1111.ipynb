{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a2a9757e885c43db9706ef42468d59c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41b9a851b841425ca642727adf3530cd",
              "IPY_MODEL_7154512d69ed4ba99bac7daa08fb3349",
              "IPY_MODEL_778c72268fd44721a61ee6f8897408c9"
            ],
            "layout": "IPY_MODEL_cf39646382da4acbb6eba8c1d9333ae7"
          }
        },
        "41b9a851b841425ca642727adf3530cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26d0f2166a3345c1afe376e71f38da5d",
            "placeholder": "​",
            "style": "IPY_MODEL_473a83c96db441a1b08ad49cfcc57bcc",
            "value": "model.safetensors: 100%"
          }
        },
        "7154512d69ed4ba99bac7daa08fb3349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_882e84a30db243b2909658c719e1c6db",
            "max": 3893998496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0f1aaef9ccf4906a466d5a85bc696f4",
            "value": 3893998496
          }
        },
        "778c72268fd44721a61ee6f8897408c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16844513a623441fa00c683ebbc10e5a",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3a7424dd084d78ba60e9fb59d29bc9",
            "value": " 3.89G/3.89G [00:33&lt;00:00, 161MB/s]"
          }
        },
        "cf39646382da4acbb6eba8c1d9333ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26d0f2166a3345c1afe376e71f38da5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473a83c96db441a1b08ad49cfcc57bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "882e84a30db243b2909658c719e1c6db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f1aaef9ccf4906a466d5a85bc696f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16844513a623441fa00c683ebbc10e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3a7424dd084d78ba60e9fb59d29bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0bbff47acaa4e93b8f78e7f896fddf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c1077484b284753a92a0be8bc570ca0",
              "IPY_MODEL_e32c2a5ec05545f6ad44cd93ebb1605a",
              "IPY_MODEL_199e1a2c640740dd8b8bc75880937ca7"
            ],
            "layout": "IPY_MODEL_6c6ba1c7dd834cd8bd3da93c7dd8d135"
          }
        },
        "7c1077484b284753a92a0be8bc570ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79fc914b3d174a43b21bc2c8e54ac910",
            "placeholder": "​",
            "style": "IPY_MODEL_b4b1e25116164f7b8277a3c1d6860fd1",
            "value": "generation_config.json: 100%"
          }
        },
        "e32c2a5ec05545f6ad44cd93ebb1605a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0c738110674315af03d52442b26c8c",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f110d12f25fa43b7af18df07864ad4a0",
            "value": 137
          }
        },
        "199e1a2c640740dd8b8bc75880937ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cddf1281b3d240e28da66b54a70bf867",
            "placeholder": "​",
            "style": "IPY_MODEL_39748dc3b6994e8ab5e8ca9cd2ad03f6",
            "value": " 137/137 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "6c6ba1c7dd834cd8bd3da93c7dd8d135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79fc914b3d174a43b21bc2c8e54ac910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b1e25116164f7b8277a3c1d6860fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d0c738110674315af03d52442b26c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f110d12f25fa43b7af18df07864ad4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cddf1281b3d240e28da66b54a70bf867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39748dc3b6994e8ab5e8ca9cd2ad03f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2635f838b52b4f6e963e3b74d08b90b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5aac8fe82cf4eca8c58ce30b067fd59",
              "IPY_MODEL_5fbc43ad19554484938e169db779ab3c",
              "IPY_MODEL_9fb2670e06934b53b8a36dc89dd30a39"
            ],
            "layout": "IPY_MODEL_e09f5bad878a4f2ea24f170d1cc2c84b"
          }
        },
        "f5aac8fe82cf4eca8c58ce30b067fd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22fe10972cd54aeba28119f27ea3216b",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f4f948ecaf4b04b978a2836a3a0a30",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "5fbc43ad19554484938e169db779ab3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af027d79162e4ff1b1a5fac3baf8b67f",
            "max": 727,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bd94a8f38a74851bac6f97c3a8d82f3",
            "value": 727
          }
        },
        "9fb2670e06934b53b8a36dc89dd30a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e630a89b4c7a486094a7f5880478dbf6",
            "placeholder": "​",
            "style": "IPY_MODEL_27f091beb375477a96bf35843c44d8e3",
            "value": " 727/727 [00:00&lt;00:00, 65.5kB/s]"
          }
        },
        "e09f5bad878a4f2ea24f170d1cc2c84b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22fe10972cd54aeba28119f27ea3216b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f4f948ecaf4b04b978a2836a3a0a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af027d79162e4ff1b1a5fac3baf8b67f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd94a8f38a74851bac6f97c3a8d82f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e630a89b4c7a486094a7f5880478dbf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27f091beb375477a96bf35843c44d8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f7a1ad718504208b8d9c9367f8daf3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc74bac5c8ed4af39ee33fec453df84c",
              "IPY_MODEL_e109458e408943d7b928bfe844227f11",
              "IPY_MODEL_ccd79d210cf7485887eedcb8d2452cfc"
            ],
            "layout": "IPY_MODEL_64da1ca68ecf46f780bc3ef14fa5b9e8"
          }
        },
        "fc74bac5c8ed4af39ee33fec453df84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38781961407b4f14a72e7a545d493aef",
            "placeholder": "​",
            "style": "IPY_MODEL_b0d64561525742c3a8907b4a67225b0d",
            "value": "tokenizer.model: 100%"
          }
        },
        "e109458e408943d7b928bfe844227f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eba1196d15c24256986db8b6bfa7707a",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3850ce561be94c538e4641843b440e23",
            "value": 499723
          }
        },
        "ccd79d210cf7485887eedcb8d2452cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4ac453fce84e70bea24eaa77d09c3f",
            "placeholder": "​",
            "style": "IPY_MODEL_7b0eda8d83c046f1b0e9871103133e45",
            "value": " 500k/500k [00:00&lt;00:00, 27.8MB/s]"
          }
        },
        "64da1ca68ecf46f780bc3ef14fa5b9e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38781961407b4f14a72e7a545d493aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d64561525742c3a8907b4a67225b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba1196d15c24256986db8b6bfa7707a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3850ce561be94c538e4641843b440e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f4ac453fce84e70bea24eaa77d09c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b0eda8d83c046f1b0e9871103133e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f106398e993476cac65293544a1c019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51b2cbe6a99f4ce58bf8b4708ac5e715",
              "IPY_MODEL_7f86f16fcfa64370ab8e5ad315c0235b",
              "IPY_MODEL_7e2855c8ae1c447398b01c51f0468562"
            ],
            "layout": "IPY_MODEL_2431a7e25ce240968e50cafcc7f9ceea"
          }
        },
        "51b2cbe6a99f4ce58bf8b4708ac5e715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c47ebacb9a48cfb2c54585ecae8864",
            "placeholder": "​",
            "style": "IPY_MODEL_4e9aa71944264eaa966d319b465d2998",
            "value": "tokenizer.json: 100%"
          }
        },
        "7f86f16fcfa64370ab8e5ad315c0235b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b76064f72c2448f1bb8335fcdb9ecb41",
            "max": 1842847,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8216e9470a6b47ebad312cdc5771043a",
            "value": 1842847
          }
        },
        "7e2855c8ae1c447398b01c51f0468562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be6bba918af41f593dd0cdae1d5835d",
            "placeholder": "​",
            "style": "IPY_MODEL_ea0c986042cf410194a1bfc9fedb1f22",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 8.50MB/s]"
          }
        },
        "2431a7e25ce240968e50cafcc7f9ceea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c47ebacb9a48cfb2c54585ecae8864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e9aa71944264eaa966d319b465d2998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b76064f72c2448f1bb8335fcdb9ecb41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8216e9470a6b47ebad312cdc5771043a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3be6bba918af41f593dd0cdae1d5835d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0c986042cf410194a1bfc9fedb1f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b5b30d49acd4a10a8636d37c097e6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf5c11fb5a9748dd93288153c246acb0",
              "IPY_MODEL_cace204cb3644c40a0f7bec80d7d2ded",
              "IPY_MODEL_f9b95f74fcf146ff803fa0ee239808a8"
            ],
            "layout": "IPY_MODEL_a253eee319074c039b05182e9c2d1eb0"
          }
        },
        "cf5c11fb5a9748dd93288153c246acb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77f9db965be437aa0ccfcf575f3a0d2",
            "placeholder": "​",
            "style": "IPY_MODEL_6795ac95f16c4f0f81677a6340b68c7a",
            "value": "added_tokens.json: 100%"
          }
        },
        "cace204cb3644c40a0f7bec80d7d2ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f67da2545b743179dd20b1cc2975b23",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08e8e618dde34396b359038134b1c50f",
            "value": 21
          }
        },
        "f9b95f74fcf146ff803fa0ee239808a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f064d6a1f8ff4626acba38c88b5b78b9",
            "placeholder": "​",
            "style": "IPY_MODEL_b7a925e1cad8468890a5d6e2a25514ad",
            "value": " 21.0/21.0 [00:00&lt;00:00, 2.57kB/s]"
          }
        },
        "a253eee319074c039b05182e9c2d1eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77f9db965be437aa0ccfcf575f3a0d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6795ac95f16c4f0f81677a6340b68c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f67da2545b743179dd20b1cc2975b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08e8e618dde34396b359038134b1c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f064d6a1f8ff4626acba38c88b5b78b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7a925e1cad8468890a5d6e2a25514ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7be777a12f974533abc2c1ae0d7fffe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a387a2128334e45897724172b9e4619",
              "IPY_MODEL_44827c9c80e74b06bcff1ba9f81acf31",
              "IPY_MODEL_87b56f6119b2414080aabbb9472dc977"
            ],
            "layout": "IPY_MODEL_467b92ed2653485fb29422c2959fd870"
          }
        },
        "7a387a2128334e45897724172b9e4619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7cb781dd904e0393c41af6c862d439",
            "placeholder": "​",
            "style": "IPY_MODEL_45b1079f310e455ab01aab4d5b8582c6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "44827c9c80e74b06bcff1ba9f81acf31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6d10f16b4e54fbdadd8810a734b8b2e",
            "max": 95,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b12ba12f85f3441e888c07887c965088",
            "value": 95
          }
        },
        "87b56f6119b2414080aabbb9472dc977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ff24f41db3c4ac3a09d6538ed480646",
            "placeholder": "​",
            "style": "IPY_MODEL_b0c6378258984f2098adfe3131ee9bff",
            "value": " 95.0/95.0 [00:00&lt;00:00, 9.25kB/s]"
          }
        },
        "467b92ed2653485fb29422c2959fd870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7cb781dd904e0393c41af6c862d439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45b1079f310e455ab01aab4d5b8582c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6d10f16b4e54fbdadd8810a734b8b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b12ba12f85f3441e888c07887c965088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ff24f41db3c4ac3a09d6538ed480646": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0c6378258984f2098adfe3131ee9bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESR5H9Uv1PyO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CN6ee8Lv1Qov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3jSdm3oh1Qrc",
        "outputId": "e13686d6-858d-4950-a6b2-00a138d0ca15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optimum\n",
            "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.11/dist-packages (from optimum) (4.50.0)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum) (2.6.0+cu124)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11->optimum) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.29->optimum) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2025.1.31)\n",
            "Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, optimum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 optimum-1.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "27d3a28ada55461a8cf356547a76c4dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXKKzeNxfed3",
        "outputId": "df68d280-ef3f-4d3d-9be6-6a655d874a4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gptqmodel\n",
            "  Downloading gptqmodel-2.1.0.tar.gz (280 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/280.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m276.5/280.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.5/280.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (1.5.2)\n",
            "Collecting datasets>=3.2.0 (from gptqmodel)\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting numpy>=2.2.2 (from gptqmodel)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.49.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (4.50.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (3.6.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (24.2)\n",
            "Collecting device-smi==0.4.1 (from gptqmodel)\n",
            "  Downloading device_smi-0.4.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=5.29.3 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (5.29.4)\n",
            "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (11.1.0)\n",
            "Collecting hf_transfer>=0.1.9 (from gptqmodel)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gptqmodel) (0.29.3)\n",
            "Collecting tokenicer==0.0.4 (from gptqmodel)\n",
            "  Downloading tokenicer-0.0.4.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting logbar==0.0.3 (from gptqmodel)\n",
            "  Downloading logbar-0.0.3.tar.gz (9.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.3.0->gptqmodel) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=1.3.0->gptqmodel) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.2.0->gptqmodel)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (4.67.1)\n",
            "Collecting xxhash (from datasets>=3.2.0->gptqmodel)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=3.2.0->gptqmodel)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.2.0->gptqmodel)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.2.0->gptqmodel) (3.11.14)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.28.1->gptqmodel) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->gptqmodel) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->gptqmodel) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->gptqmodel) (0.21.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.2.0->gptqmodel) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.2.0->gptqmodel) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.2.0->gptqmodel) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.2.0->gptqmodel) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.2.0->gptqmodel) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->gptqmodel) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.2.0->gptqmodel) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.2.0->gptqmodel) (1.17.0)\n",
            "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gptqmodel, device-smi, logbar, tokenicer\n",
            "  Building wheel for gptqmodel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gptqmodel: filename=gptqmodel-2.1.0-cp311-cp311-linux_x86_64.whl size=3199595 sha256=ba78494e53c5ae253157b117d9ed1073b3ec4cde27a78b701f33cfabc4ff8b6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/3b/2d/8242df8df3a8191165ee9d4b8a733e455f3163805723376e67\n",
            "  Building wheel for device-smi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for device-smi: filename=device_smi-0.4.1-py3-none-any.whl size=17887 sha256=6d8c1ca6cf0e6ecc586664a0f6beda37d7f078e9b0348bb061ebc8068fe43dea\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/82/37/d8751fd90fc238023fe6d5e3c7d7deedb07f03a3ed7264aca2\n",
            "  Building wheel for logbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for logbar: filename=logbar-0.0.3-py3-none-any.whl size=11226 sha256=15314883d925d34999e0beb3a2f3877ebdd1688a315e62a042bd1fbf2179a2b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/f5/8a/d9b8fd9ea865992df78a683f52df5ea2dc8d8f2501d3ed23ba\n",
            "  Building wheel for tokenicer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tokenicer: filename=tokenicer-0.0.4-py3-none-any.whl size=11354 sha256=ca065ace597f9e9be2c0b1e7e3280d2220257b5868d909da5def5b50183a29ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/72/04/60d66d0e3840894a498c4bc5536884b23bfbeb308308037d84\n",
            "Successfully built gptqmodel device-smi logbar tokenicer\n",
            "Installing collected packages: xxhash, numpy, logbar, hf_transfer, fsspec, dill, device-smi, multiprocess, datasets, tokenicer, gptqmodel\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.4.1 device-smi-0.4.1 dill-0.3.8 fsspec-2024.12.0 gptqmodel-2.1.0 hf_transfer-0.1.9 logbar-0.0.3 multiprocess-0.70.16 numpy-2.2.4 tokenicer-0.0.4 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade numpy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhWHirlOghdY",
        "outputId": "369f87e5-feb8-436e-98de-7fb9afe141e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.4)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install --upgrade --force-reinstall numpy"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzzT0NJJ2Yyl",
        "outputId": "150e39d7-791a-4599-cace-f709608520b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model_name = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=11,\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895,
          "referenced_widgets": [
            "a2a9757e885c43db9706ef42468d59c6",
            "41b9a851b841425ca642727adf3530cd",
            "7154512d69ed4ba99bac7daa08fb3349",
            "778c72268fd44721a61ee6f8897408c9",
            "cf39646382da4acbb6eba8c1d9333ae7",
            "26d0f2166a3345c1afe376e71f38da5d",
            "473a83c96db441a1b08ad49cfcc57bcc",
            "882e84a30db243b2909658c719e1c6db",
            "a0f1aaef9ccf4906a466d5a85bc696f4",
            "16844513a623441fa00c683ebbc10e5a",
            "4d3a7424dd084d78ba60e9fb59d29bc9",
            "a0bbff47acaa4e93b8f78e7f896fddf3",
            "7c1077484b284753a92a0be8bc570ca0",
            "e32c2a5ec05545f6ad44cd93ebb1605a",
            "199e1a2c640740dd8b8bc75880937ca7",
            "6c6ba1c7dd834cd8bd3da93c7dd8d135",
            "79fc914b3d174a43b21bc2c8e54ac910",
            "b4b1e25116164f7b8277a3c1d6860fd1",
            "8d0c738110674315af03d52442b26c8c",
            "f110d12f25fa43b7af18df07864ad4a0",
            "cddf1281b3d240e28da66b54a70bf867",
            "39748dc3b6994e8ab5e8ca9cd2ad03f6",
            "2635f838b52b4f6e963e3b74d08b90b9",
            "f5aac8fe82cf4eca8c58ce30b067fd59",
            "5fbc43ad19554484938e169db779ab3c",
            "9fb2670e06934b53b8a36dc89dd30a39",
            "e09f5bad878a4f2ea24f170d1cc2c84b",
            "22fe10972cd54aeba28119f27ea3216b",
            "f7f4f948ecaf4b04b978a2836a3a0a30",
            "af027d79162e4ff1b1a5fac3baf8b67f",
            "9bd94a8f38a74851bac6f97c3a8d82f3",
            "e630a89b4c7a486094a7f5880478dbf6",
            "27f091beb375477a96bf35843c44d8e3",
            "0f7a1ad718504208b8d9c9367f8daf3b",
            "fc74bac5c8ed4af39ee33fec453df84c",
            "e109458e408943d7b928bfe844227f11",
            "ccd79d210cf7485887eedcb8d2452cfc",
            "64da1ca68ecf46f780bc3ef14fa5b9e8",
            "38781961407b4f14a72e7a545d493aef",
            "b0d64561525742c3a8907b4a67225b0d",
            "eba1196d15c24256986db8b6bfa7707a",
            "3850ce561be94c538e4641843b440e23",
            "2f4ac453fce84e70bea24eaa77d09c3f",
            "7b0eda8d83c046f1b0e9871103133e45",
            "4f106398e993476cac65293544a1c019",
            "51b2cbe6a99f4ce58bf8b4708ac5e715",
            "7f86f16fcfa64370ab8e5ad315c0235b",
            "7e2855c8ae1c447398b01c51f0468562",
            "2431a7e25ce240968e50cafcc7f9ceea",
            "83c47ebacb9a48cfb2c54585ecae8864",
            "4e9aa71944264eaa966d319b465d2998",
            "b76064f72c2448f1bb8335fcdb9ecb41",
            "8216e9470a6b47ebad312cdc5771043a",
            "3be6bba918af41f593dd0cdae1d5835d",
            "ea0c986042cf410194a1bfc9fedb1f22",
            "5b5b30d49acd4a10a8636d37c097e6a9",
            "cf5c11fb5a9748dd93288153c246acb0",
            "cace204cb3644c40a0f7bec80d7d2ded",
            "f9b95f74fcf146ff803fa0ee239808a8",
            "a253eee319074c039b05182e9c2d1eb0",
            "c77f9db965be437aa0ccfcf575f3a0d2",
            "6795ac95f16c4f0f81677a6340b68c7a",
            "0f67da2545b743179dd20b1cc2975b23",
            "08e8e618dde34396b359038134b1c50f",
            "f064d6a1f8ff4626acba38c88b5b78b9",
            "b7a925e1cad8468890a5d6e2a25514ad",
            "7be777a12f974533abc2c1ae0d7fffe5",
            "7a387a2128334e45897724172b9e4619",
            "44827c9c80e74b06bcff1ba9f81acf31",
            "87b56f6119b2414080aabbb9472dc977",
            "467b92ed2653485fb29422c2959fd870",
            "6b7cb781dd904e0393c41af6c862d439",
            "45b1079f310e455ab01aab4d5b8582c6",
            "b6d10f16b4e54fbdadd8810a734b8b2e",
            "b12ba12f85f3441e888c07887c965088",
            "0ff24f41db3c4ac3a09d6538ed480646",
            "b0c6378258984f2098adfe3131ee9bff"
          ]
        },
        "id": "Ur3xM4bs1QuV",
        "outputId": "f96680c3-d5f7-422d-a037-5c936b548ca0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.89G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2a9757e885c43db9706ef42468d59c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0bbff47acaa4e93b8f78e7f896fddf3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.04667949676513672s                                            \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2635f838b52b4f6e963e3b74d08b90b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f7a1ad718504208b8d9c9367f8daf3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f106398e993476cac65293544a1c019"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b5b30d49acd4a10a8636d37c097e6a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7be777a12f974533abc2c1ae0d7fffe5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3e0fc903c0bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m ]\n\u001b[0;32m---> 16\u001b[0;31m text = tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{\\%-?\\s*generation\\s*-?\\%\\}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1823\u001b[0m                     \u001b[0;34m\"Cannot use chat template functions because tokenizer.chat_template is not set and no template \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;34m\"argument was passed! For information about writing templates and setting the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "# Define a chat template\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content']}}\"\"\"\n",
        "\n",
        "# Pass the chat template to apply_chat_template\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    chat_template=chat_template, # Pass the template here\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=11,\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dImo7fE222k-",
        "outputId": "4317adc2-7246-462e-a315-39928fe9298d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.04560589790344238s                                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sure, I can help you with that.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"TheBloke/WizardLM-7B-uncensored-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "# Define a chat template\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content']}}\"\"\"\n",
        "\n",
        "# Pass the chat template to apply_chat_template\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    chat_template=chat_template, # Pass the template here\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU_ge1bV3VfG",
        "outputId": "da6f974f-e55f-4530-ef5f-18a04206c3dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.020236730575561523s                                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sure, I can give you an introduction to large language model. A language model is a computer program that uses artificial intelligence to understand and generate human-like speech. It is used to create conversations with the user. The language model is designed to recognize and understand human language, and it can be used to improve the natural language processing of the machine. It is a powerful tool for improving the language processing of the machine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rdfq53pV3Vb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSYIm8Up3VVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"rakmik/lama3.21b-gptq\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Give me a short introduction to large language model.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "# Define a chat template\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content']}}\"\"\"\n",
        "\n",
        "# Pass the chat template to apply_chat_template\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    chat_template=chat_template, # Pass the template here\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=11,\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "nYFuyuKv3AKF",
        "outputId": "e6d3f0e8-1817-4891-a4f2-397c0187b0fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.015100479125976562s                                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ab0314f742ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2327\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"rakmik/lama3.21b-gptq\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "text = \"Hello my name is\"\n",
        "\n",
        "# Instead of directly using .to(0), use the model's device\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "WfDuom0r5YhN",
        "outputId": "1c050535-a66c-4adf-f556-4a92e07ff367"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `ExllamaQuantLinear`                               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.013217926025390625s                                           \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "no device index",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-da3b79c326c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rakmik/lama3.21b-gptq\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello my name is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    574\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4536\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4537\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_quantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/base.py\u001b[0m in \u001b[0;36mpostprocess_model\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0malong\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_process_model_after_weight_loading\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_model_after_weight_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdequantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_gptq.py\u001b[0m in \u001b[0;36m_process_model_after_weight_loading\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_model_after_weight_loading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"PreTrainedModel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_quantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimum_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optimum/gptq/quantizer.py\u001b[0m in \u001b[0;36mpost_init_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStoreAttr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgptq_post_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_act_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         if (\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/utils/model.py\u001b[0m in \u001b[0;36mhf_gptqmodel_post_init\u001b[0;34m(model, use_act_order, quantize_config, max_input_length)\u001b[0m\n\u001b[1;32m    687\u001b[0m def hf_gptqmodel_post_init(model, use_act_order: bool, quantize_config: QuantizeConfig = None,\n\u001b[1;32m    688\u001b[0m                         max_input_length: Optional[int] = None):\n\u001b[0;32m--> 689\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgptqmodel_post_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_act_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantize_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gptqmodel/utils/model.py\u001b[0m in \u001b[0;36mgptqmodel_post_init\u001b[0;34m(model, use_act_order, quantize_config, max_input_length)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_to_buffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             \u001b[0mprepare_buffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temp_state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temp_dq\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;31m# Using the default from exllama repo here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: no device index"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"rakmik/lama3.21b-gptq\"\n",
        "# Load the model on CPU by setting device_map=\"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "text = \"Hello my name is\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "c2jnSuuK5jiW",
        "outputId": "59a9a19a-8519-458d-f67b-6077fcc1d1fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.010191202163696289s                                           \n",
            "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0fd8b46bb37a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2327\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"rakmik/lama3.21b-gptq\"\n",
        "# Load the model on CPU by setting device_map=\"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "text = \"Hello my name is\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Apply temperature scaling to the logits to avoid extreme probabilities\n",
        "out = model.generate(**inputs, max_new_tokens=5, temperature=0.7)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "26AlHOKE6El9",
        "outputId": "479d5bf1-9b42-4121-f402-95f221680af5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n",
            "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.010511159896850586s                                           \n",
            "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9124d9101be7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Apply temperature scaling to the logits to avoid extreme probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2327\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"rakmik/lama3.21b-gptq\"\n",
        "# Load the model on CPU by setting device_map=\"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "text = \"Hello my name is\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Apply temperature scaling and adjust parameters to avoid extreme probabilities\n",
        "out = model.generate(**inputs, max_new_tokens=5, temperature=0.7, num_beams=1, no_repeat_ngram_size=2)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "X0SOgnJm6l3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"rakmik/lama3.21b-gptq\")\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "3_09XZJ26eOO",
        "outputId": "992e3f56-37e4-437f-ccab-8472371cd638"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.015112876892089844s                                           \n",
            "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b07ace7e75bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rakmik/lama3.21b-gptq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 🐈 🐈 🐈\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_final_message\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mcontinue_final_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             inputs = self.tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mprompt_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{\\%-?\\s*generation\\s*-?\\%\\}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1823\u001b[0m                     \u001b[0;34m\"Cannot use chat template functions because tokenizer.chat_template is not set and no template \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;34m\"argument was passed! For information about writing templates and setting the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s-hiLeun6u_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"rakmik/lama3.21b-gptq\")\n",
        "\n",
        "# Define a chat template\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content'] if messages[1]['role'] == 'user' else ''}}{% if messages[1]['role'] == 'assistant' %}{{messages[1]['content']}}{% endif %}\"\"\"\n",
        "\n",
        "# Pass the chat template to the pipeline\n",
        "pipe(messages, chat_template=chat_template)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "N6DH-YKT6xL2",
        "outputId": "5555d8fb-73e0-4a2f-b33f-8e7b2bf3f03d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.023538827896118164s                                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-56cefbea2b0a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Pass the chat template to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 🐈 🐈 🐈\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_final_message\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mcontinue_final_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"assistant\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             inputs = self.tokenizer.apply_chat_template(\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mprompt_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mcontinue_final_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_assistant_tokens_mask\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{\\%-?\\s*generation\\s*-?\\%\\}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mget_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m                 \u001b[0mchat_template\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1823\u001b[0m                     \u001b[0;34m\"Cannot use chat template functions because tokenizer.chat_template is not set and no template \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;34m\"argument was passed! For information about writing templates and setting the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
          ]
        }
      ]
    },
    {
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"rakmik/lama3.21b-gptq\")\n",
        "\n",
        "# Define a chat template\n",
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content'] if messages[1]['role'] == 'user' else ''}}{% if messages[1]['role'] == 'assistant' %}{{messages[1]['content']}}{% endif %}\"\"\"\n",
        "\n",
        "# Apply the chat template manually using the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rakmik/lama3.21b-gptq\")\n",
        "text = tokenizer.apply_chat_template(messages, chat_template=chat_template, tokenize=False)\n",
        "\n",
        "# Pass the formatted text to the pipeline\n",
        "pipe(text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "fwfX72Ox64zG",
        "outputId": "2bbd087b-93a8-4599-bc4c-365478217cd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.012221813201904297s                                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UndefinedError",
          "evalue": "list object has no element 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b0bc1c0687e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rakmik/lama3.21b-gptq\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Pass the formatted text to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mapply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                 \u001b[0mall_generation_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m                 rendered_chat = compiled_template.render(\n\u001b[0m\u001b[1;32m   1696\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_schemas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_render_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrender_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/environment.py\u001b[0m in \u001b[0;36mhandle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrewrite_traceback_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrewrite_traceback_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<template>\u001b[0m in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jinja2/sandbox.py\u001b[0m in \u001b[0;36mgetitem\u001b[0;34m(self, obj, argument)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;34m\"\"\"Subscribe an object from sandboxed code.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUndefinedError\u001b[0m: list object has no element 1"
          ]
        }
      ]
    },
    {
      "source": [
        "chat_template = \"\"\"{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}\n",
        "{{messages[1]['content'] if messages[1]['role'] == 'user' else ''}}{% if messages[1]['role'] == 'assistant' %}{{messages[1]['content']}}{% endif %}\"\"\""
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EzpHST7T7AIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "csKOsAyM7AYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"rakmik/lama3.21b-gptq\")\n",
        "\n",
        "# Define a chat template\n",
        "# The original template expected a second message with 'system' or 'assistant' role\n",
        "# This modified template only accesses the first message (index 0)\n",
        "chat_template = \"\"\"{{messages[0]['content']}}\"\"\"\n",
        "\n",
        "# Apply the chat template manually using the tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rakmik/lama3.21b-gptq\")\n",
        "text = tokenizer.apply_chat_template(messages, chat_template=chat_template, tokenize=False)\n",
        "\n",
        "# Pass the formatted text to the pipeline\n",
        "pipe(text)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "a5XlXpqF7EtW",
        "outputId": "e5e25ee0-219d-43b6-d4a1-a33d6c04c3a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.010401487350463867s                                           \n",
            "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22dc539ac227>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Pass the formatted text to the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2327\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSM1Md2W7ohQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"rakmik/lama3.21b-gptq\"\n",
        "pipe = pipeline(\"text-generation\", model=model_id)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "# Define a chat template to use the content of the message\n",
        "chat_template = \"\"\"{{messages[0]['content']}}\"\"\"\n",
        "\n",
        "text = tokenizer.apply_chat_template(messages, chat_template=chat_template, tokenize=False)\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(pipe.device)\n",
        "\n",
        "# Check input_ids if there are \"nan\" values\n",
        "nan_indices = torch.isnan(inputs.input_ids)\n",
        "if nan_indices.any():\n",
        "    print(\"input_ids has NaN values at: \", nan_indices.nonzero())\n",
        "    # Replace NaN values with 0\n",
        "    inputs.input_ids[nan_indices] = 0\n",
        "\n",
        "# Check input_ids if there are \"inf\" values\n",
        "inf_indices = torch.isinf(inputs.input_ids)\n",
        "if inf_indices.any():\n",
        "    print(\"input_ids has inf values at: \", inf_indices.nonzero())\n",
        "    # Replace inf values with 0\n",
        "    inputs.input_ids[inf_indices] = 0\n",
        "\n",
        "# Check if any values in input_ids are outside the range of float16\n",
        "out_of_range = torch.logical_or(inputs.input_ids > torch.finfo(torch.float16).max, inputs.input_ids < torch.finfo(torch.float16).min)\n",
        "if out_of_range.any():\n",
        "    print(\"input_ids has values outside float16 range at: \", out_of_range.nonzero())\n",
        "    # Replace out-of-range values with 0\n",
        "    inputs.input_ids[out_of_range] = 0\n",
        "\n",
        "# Apply the pipeline for text generation\n",
        "out = pipe(text)  # remove **inputs for pipe\n",
        "\n",
        "print(out)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "6Uc9zK7K7pzG",
        "outputId": "e63a3725-9e22-4a22-e138-2f694b9f29a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.      \n",
            "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.                              \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`                                 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.                    \n",
            "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v1 to v2                                                             \n",
            "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.010911226272583008s                                           \n",
            "\u001b[32mINFO\u001b[0m  Optimize: `TorchQuantLinear` compilation triggered.                                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids has values outside float16 range at:  tensor([[0, 0]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0b362b36b709>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Apply the pipeline for text generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove **inputs for pipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             )\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2327\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3330\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m                 \u001b[0;31m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3332\u001b[0;31m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m                 \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AutoGPTQ/AutoGPTQ.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQj6rzwR7zKe",
        "outputId": "a094351f-fec8-4a0e-b87e-359e6bb89fc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AutoGPTQ'...\n",
            "remote: Enumerating objects: 5012, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 5012 (delta 37), reused 15 (delta 15), pack-reused 4957 (from 5)\u001b[K\n",
            "Receiving objects: 100% (5012/5012), 8.22 MiB | 11.71 MiB/s, done.\n",
            "Resolving deltas: 100% (3284/3284), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/quantization\n",
        "!python /content/AutoGPTQ/examples/quantization/quant_with_alpaca.py"
      ],
      "metadata": {
        "id": "wDrGopgd8H9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/quantization\n",
        "!python quant_with_alpaca.py --pretrained_model_dir \"facebook/opt-125m\" --per_gpu_max_memory 1 --quant_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eruboFK8b03",
        "outputId": "a61931e8-7760-4c50-e757-47b4ae2e8aa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/quantization\n",
            "2025-03-26 05:37:00.606250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742967420.642624   13684 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742967420.653517   13684 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "tokenizer_config.json: 100% 685/685 [00:00<00:00, 3.40MB/s]\n",
            "config.json: 100% 651/651 [00:00<00:00, 3.45MB/s]\n",
            "vocab.json: 100% 899k/899k [00:00<00:00, 4.08MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 4.19MB/s]\n",
            "special_tokens_map.json: 100% 441/441 [00:00<00:00, 2.85MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "pytorch_model.bin: 100% 251M/251M [00:01<00:00, 181MB/s]\n",
            "generation_config.json: 100% 137/137 [00:00<00:00, 1.20MB/s]\n",
            "model.safetensors:   0% 0.00/251M [00:00<?, ?B/s]\n",
            "Generating train split: 128 examples [00:00, 4142.30 examples/s]\n",
            "\n",
            "model.safetensors:   8% 21.0M/251M [00:01<00:12, 18.5MB/s]\n",
            "Map: 100% 128/128 [00:00<00:00, 489.80 examples/s]\n",
            "model.safetensors:  29% 73.4M/251M [00:01<00:02, 79.8MB/s]INFO - Start quantizing layer 1/12\n",
            "2025-03-26 05:37:15 INFO [auto_gptq.modeling._base] Start quantizing layer 1/12\n",
            "model.safetensors:  46% 115M/251M [00:01<00:01, 106MB/s] INFO - Quantizing self_attn.k_proj in layer 1/12...\n",
            "2025-03-26 05:37:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/12...\n",
            "model.safetensors:  96% 241M/251M [00:03<00:00, 103MB/s]2025-03-26 05:37:16 INFO [auto_gptq.quantization.gptq] duration: 1.0693633556365967\n",
            "2025-03-26 05:37:16 INFO [auto_gptq.quantization.gptq] avg loss: 2.305187225341797\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/12...\n",
            "2025-03-26 05:37:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/12...\n",
            "model.safetensors: 100% 251M/251M [00:03<00:00, 77.8MB/s]\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] duration: 0.360684871673584\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.038932494819164276\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/12...\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/12...\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] duration: 0.3099055290222168\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] avg loss: 1.8174734115600586\n",
            "INFO - Quantizing self_attn.out_proj in layer 1/12...\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 1/12...\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] duration: 0.36288022994995117\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.000995906419120729\n",
            "INFO - Quantizing fc1 in layer 1/12...\n",
            "2025-03-26 05:37:17 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 1/12...\n",
            "2025-03-26 05:37:18 INFO [auto_gptq.quantization.gptq] duration: 0.3382890224456787\n",
            "2025-03-26 05:37:18 INFO [auto_gptq.quantization.gptq] avg loss: 35.366477966308594\n",
            "INFO - Quantizing fc2 in layer 1/12...\n",
            "2025-03-26 05:37:18 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 1/12...\n",
            "2025-03-26 05:37:19 INFO [auto_gptq.quantization.gptq] duration: 1.43172287940979\n",
            "2025-03-26 05:37:19 INFO [auto_gptq.quantization.gptq] avg loss: 4.906728744506836\n",
            "INFO - Start quantizing layer 2/12\n",
            "2025-03-26 05:37:19 INFO [auto_gptq.modeling._base] Start quantizing layer 2/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/12...\n",
            "2025-03-26 05:37:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] duration: 0.3388981819152832\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] avg loss: 1.3507639169692993\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] duration: 0.2850039005279541\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.043117135763168335\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] duration: 0.32251644134521484\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] avg loss: 1.0190155506134033\n",
            "INFO - Quantizing self_attn.out_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] duration: 0.3176298141479492\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.0010825194185599685\n",
            "INFO - Quantizing fc1 in layer 2/12...\n",
            "2025-03-26 05:37:20 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 2/12...\n",
            "2025-03-26 05:37:21 INFO [auto_gptq.quantization.gptq] duration: 0.3040018081665039\n",
            "2025-03-26 05:37:21 INFO [auto_gptq.quantization.gptq] avg loss: 160.3538055419922\n",
            "INFO - Quantizing fc2 in layer 2/12...\n",
            "2025-03-26 05:37:21 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 2/12...\n",
            "2025-03-26 05:37:22 INFO [auto_gptq.quantization.gptq] duration: 1.5296525955200195\n",
            "2025-03-26 05:37:22 INFO [auto_gptq.quantization.gptq] avg loss: 1.2419228553771973\n",
            "INFO - Start quantizing layer 3/12\n",
            "2025-03-26 05:37:22 INFO [auto_gptq.modeling._base] Start quantizing layer 3/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/12...\n",
            "2025-03-26 05:37:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/12...\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.quantization.gptq] duration: 0.4050915241241455\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.quantization.gptq] avg loss: 2.8433432579040527\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/12...\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/12...\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.quantization.gptq] duration: 0.3845517635345459\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.14139601588249207\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/12...\n",
            "2025-03-26 05:37:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] duration: 0.448927640914917\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] avg loss: 2.3712430000305176\n",
            "INFO - Quantizing self_attn.out_proj in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] duration: 0.3970963954925537\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.0013048514956608415\n",
            "INFO - Quantizing fc1 in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] duration: 0.328859806060791\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.quantization.gptq] avg loss: 164.13375854492188\n",
            "INFO - Quantizing fc2 in layer 3/12...\n",
            "2025-03-26 05:37:24 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 3/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] duration: 1.3847410678863525\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] avg loss: 1.8498977422714233\n",
            "INFO - Start quantizing layer 4/12\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.modeling._base] Start quantizing layer 4/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] duration: 0.3215920925140381\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] avg loss: 2.108827590942383\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] duration: 0.2988860607147217\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.18851493299007416\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/12...\n",
            "2025-03-26 05:37:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] duration: 0.2898099422454834\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] avg loss: 2.0279133319854736\n",
            "INFO - Quantizing self_attn.out_proj in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] duration: 0.30744123458862305\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.0013609271263703704\n",
            "INFO - Quantizing fc1 in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] duration: 0.31140637397766113\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.quantization.gptq] avg loss: 101.88461303710938\n",
            "INFO - Quantizing fc2 in layer 4/12...\n",
            "2025-03-26 05:37:27 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 4/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] duration: 1.3028502464294434\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.8624041676521301\n",
            "INFO - Start quantizing layer 5/12\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.modeling._base] Start quantizing layer 5/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] duration: 0.32166314125061035\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] avg loss: 2.6668875217437744\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] duration: 0.2983682155609131\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.2254505455493927\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/12...\n",
            "2025-03-26 05:37:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] duration: 0.30794596672058105\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] avg loss: 2.626002073287964\n",
            "INFO - Quantizing self_attn.out_proj in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] duration: 0.3141155242919922\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.0019808756187558174\n",
            "INFO - Quantizing fc1 in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] duration: 0.3032402992248535\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.quantization.gptq] avg loss: 155.47267150878906\n",
            "INFO - Quantizing fc2 in layer 5/12...\n",
            "2025-03-26 05:37:30 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 5/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] duration: 1.3357751369476318\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] avg loss: 2.600097894668579\n",
            "INFO - Start quantizing layer 6/12\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.modeling._base] Start quantizing layer 6/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] duration: 0.32677745819091797\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] avg loss: 2.796332359313965\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] duration: 0.2910730838775635\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.19185753166675568\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/12...\n",
            "2025-03-26 05:37:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] duration: 0.294586181640625\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] avg loss: 2.770320415496826\n",
            "INFO - Quantizing self_attn.out_proj in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] duration: 0.3047962188720703\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.002847518539056182\n",
            "INFO - Quantizing fc1 in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] duration: 0.32413172721862793\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.quantization.gptq] avg loss: 129.30709838867188\n",
            "INFO - Quantizing fc2 in layer 6/12...\n",
            "2025-03-26 05:37:33 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 6/12...\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.quantization.gptq] duration: 1.6316149234771729\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.quantization.gptq] avg loss: 3.571140766143799\n",
            "INFO - Start quantizing layer 7/12\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.modeling._base] Start quantizing layer 7/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/12...\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/12...\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.quantization.gptq] duration: 0.44061827659606934\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.quantization.gptq] avg loss: 2.941141128540039\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/12...\n",
            "2025-03-26 05:37:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] duration: 0.4570443630218506\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.2547203302383423\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] duration: 0.3879678249359131\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] avg loss: 2.6578404903411865\n",
            "INFO - Quantizing self_attn.out_proj in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] duration: 0.30593276023864746\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.003694383893162012\n",
            "INFO - Quantizing fc1 in layer 7/12...\n",
            "2025-03-26 05:37:36 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 7/12...\n",
            "2025-03-26 05:37:37 INFO [auto_gptq.quantization.gptq] duration: 0.3159968852996826\n",
            "2025-03-26 05:37:37 INFO [auto_gptq.quantization.gptq] avg loss: 150.74314880371094\n",
            "INFO - Quantizing fc2 in layer 7/12...\n",
            "2025-03-26 05:37:37 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 7/12...\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.quantization.gptq] duration: 1.2909455299377441\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.quantization.gptq] avg loss: 4.491937637329102\n",
            "INFO - Start quantizing layer 8/12\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.modeling._base] Start quantizing layer 8/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/12...\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/12...\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.quantization.gptq] duration: 0.32173657417297363\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.quantization.gptq] avg loss: 3.3064098358154297\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/12...\n",
            "2025-03-26 05:37:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] duration: 0.29649901390075684\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.28001338243484497\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] duration: 0.2882397174835205\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] avg loss: 3.0842368602752686\n",
            "INFO - Quantizing self_attn.out_proj in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] duration: 0.3069901466369629\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.00486025121062994\n",
            "INFO - Quantizing fc1 in layer 8/12...\n",
            "2025-03-26 05:37:39 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 8/12...\n",
            "2025-03-26 05:37:40 INFO [auto_gptq.quantization.gptq] duration: 0.32489967346191406\n",
            "2025-03-26 05:37:40 INFO [auto_gptq.quantization.gptq] avg loss: 169.65391540527344\n",
            "INFO - Quantizing fc2 in layer 8/12...\n",
            "2025-03-26 05:37:40 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 8/12...\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.quantization.gptq] duration: 1.3040359020233154\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.quantization.gptq] avg loss: 5.107122421264648\n",
            "INFO - Start quantizing layer 9/12\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.modeling._base] Start quantizing layer 9/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/12...\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/12...\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.quantization.gptq] duration: 0.32437753677368164\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.quantization.gptq] avg loss: 3.3175039291381836\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/12...\n",
            "2025-03-26 05:37:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] duration: 0.28879237174987793\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.38935357332229614\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] duration: 0.3056495189666748\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] avg loss: 3.2770376205444336\n",
            "INFO - Quantizing self_attn.out_proj in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] duration: 0.3166341781616211\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.006030519027262926\n",
            "INFO - Quantizing fc1 in layer 9/12...\n",
            "2025-03-26 05:37:42 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 9/12...\n",
            "2025-03-26 05:37:43 INFO [auto_gptq.quantization.gptq] duration: 0.3263571262359619\n",
            "2025-03-26 05:37:43 INFO [auto_gptq.quantization.gptq] avg loss: 217.13294982910156\n",
            "INFO - Quantizing fc2 in layer 9/12...\n",
            "2025-03-26 05:37:43 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 9/12...\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.quantization.gptq] duration: 1.3074567317962646\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.quantization.gptq] avg loss: 7.709597587585449\n",
            "INFO - Start quantizing layer 10/12\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.modeling._base] Start quantizing layer 10/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/12...\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/12...\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.quantization.gptq] duration: 0.3333916664123535\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.quantization.gptq] avg loss: 3.5387182235717773\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/12...\n",
            "2025-03-26 05:37:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] duration: 0.28682446479797363\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.4421749413013458\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] duration: 0.2872309684753418\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] avg loss: 3.368974208831787\n",
            "INFO - Quantizing self_attn.out_proj in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] duration: 0.3092613220214844\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.011924456804990768\n",
            "INFO - Quantizing fc1 in layer 10/12...\n",
            "2025-03-26 05:37:45 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 10/12...\n",
            "2025-03-26 05:37:46 INFO [auto_gptq.quantization.gptq] duration: 0.3249499797821045\n",
            "2025-03-26 05:37:46 INFO [auto_gptq.quantization.gptq] avg loss: 276.10626220703125\n",
            "INFO - Quantizing fc2 in layer 10/12...\n",
            "2025-03-26 05:37:46 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 10/12...\n",
            "2025-03-26 05:37:47 INFO [auto_gptq.quantization.gptq] duration: 1.605499505996704\n",
            "2025-03-26 05:37:47 INFO [auto_gptq.quantization.gptq] avg loss: 10.721729278564453\n",
            "INFO - Start quantizing layer 11/12\n",
            "2025-03-26 05:37:47 INFO [auto_gptq.modeling._base] Start quantizing layer 11/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/12...\n",
            "2025-03-26 05:37:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] duration: 0.4425668716430664\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] avg loss: 3.7499611377716064\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] duration: 0.4401235580444336\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.5434613823890686\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] duration: 0.35753297805786133\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.quantization.gptq] avg loss: 3.0873618125915527\n",
            "INFO - Quantizing self_attn.out_proj in layer 11/12...\n",
            "2025-03-26 05:37:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 11/12...\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.quantization.gptq] duration: 0.2986462116241455\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.011739108711481094\n",
            "INFO - Quantizing fc1 in layer 11/12...\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 11/12...\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.quantization.gptq] duration: 0.31159186363220215\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.quantization.gptq] avg loss: 334.46807861328125\n",
            "INFO - Quantizing fc2 in layer 11/12...\n",
            "2025-03-26 05:37:49 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 11/12...\n",
            "2025-03-26 05:37:50 INFO [auto_gptq.quantization.gptq] duration: 1.3422744274139404\n",
            "2025-03-26 05:37:50 INFO [auto_gptq.quantization.gptq] avg loss: 18.45205307006836\n",
            "INFO - Start quantizing layer 12/12\n",
            "2025-03-26 05:37:50 INFO [auto_gptq.modeling._base] Start quantizing layer 12/12\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/12...\n",
            "2025-03-26 05:37:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] duration: 0.33321332931518555\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.362189769744873\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] duration: 0.28519701957702637\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.7218361496925354\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] duration: 0.3000447750091553\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.quantization.gptq] avg loss: 3.1968891620635986\n",
            "INFO - Quantizing self_attn.out_proj in layer 12/12...\n",
            "2025-03-26 05:37:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.out_proj in layer 12/12...\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.quantization.gptq] duration: 0.3069770336151123\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.02009410597383976\n",
            "INFO - Quantizing fc1 in layer 12/12...\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.modeling._base] Quantizing fc1 in layer 12/12...\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.quantization.gptq] duration: 0.3128843307495117\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.quantization.gptq] avg loss: 493.3936767578125\n",
            "INFO - Quantizing fc2 in layer 12/12...\n",
            "2025-03-26 05:37:52 INFO [auto_gptq.modeling._base] Quantizing fc2 in layer 12/12...\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.quantization.gptq] duration: 1.2836170196533203\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.quantization.gptq] avg loss: 23.395061492919922\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.modeling._utils] Packing model...\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.self_attn.k_proj\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.self_attn.v_proj\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.self_attn.q_proj\n",
            "2025-03-26 05:37:53 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.self_attn.out_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.fc1\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.0.fc2\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.self_attn.k_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.self_attn.v_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.self_attn.q_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.self_attn.out_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.fc1\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.1.fc2\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.self_attn.k_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.self_attn.v_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.self_attn.q_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.self_attn.out_proj\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.fc1\n",
            "2025-03-26 05:37:54 INFO [auto_gptq.modeling._utils] model.decoder.layers.2.fc2\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.self_attn.k_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.self_attn.v_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.self_attn.q_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.self_attn.out_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.fc1\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.3.fc2\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.self_attn.k_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.self_attn.v_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.self_attn.q_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.self_attn.out_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.fc1\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.4.fc2\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.self_attn.k_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.self_attn.v_proj\n",
            "2025-03-26 05:37:55 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.self_attn.q_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.self_attn.out_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.fc1\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.5.fc2\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.self_attn.k_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.self_attn.v_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.self_attn.q_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.self_attn.out_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.fc1\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.6.fc2\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.self_attn.k_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.self_attn.v_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.self_attn.q_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.self_attn.out_proj\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.fc1\n",
            "2025-03-26 05:37:56 INFO [auto_gptq.modeling._utils] model.decoder.layers.7.fc2\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.self_attn.k_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.self_attn.v_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.self_attn.q_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.self_attn.out_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.fc1\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.8.fc2\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.self_attn.k_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.self_attn.v_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.self_attn.q_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.self_attn.out_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.fc1\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.9.fc2\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.self_attn.k_proj\n",
            "2025-03-26 05:37:57 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.self_attn.v_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.self_attn.q_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.self_attn.out_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.fc1\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.10.fc2\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.self_attn.k_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.self_attn.v_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.self_attn.q_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.self_attn.out_proj\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.fc1\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] model.decoder.layers.11.fc2\n",
            "2025-03-26 05:37:58 INFO [auto_gptq.modeling._utils] Model packed.\n",
            "quantization took:  43.8607s\n",
            "Device set to use cuda:0\n",
            "The model 'OPTGPTQForCausalLM' is not supported for . Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
            "prompt: Instruction:\n",
            "Suggest 3 alternative websites for the given website.\n",
            "Input:\n",
            "Wikipedia.org\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Alternative Websites: Encyclopaedia Britannica (britannica.com), Merriam-Webster Online Dictionary (merriam-webster.com), Encyclopedia of Life (eol.org).\n",
            "------------------------------------------\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "quant: \n",
            "The following is a list of the following websites that are currently in the search engine.\n",
            "\n",
            "1.\n",
            "\n",
            "The Internet Archive\n",
            "\n",
            "The Internet Archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives. The archive is a collection of the Internet Archive's archives\n",
            "generate 174 tokens using  4.9302s, 35.29296849611424 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Find the name of the capital city of France\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: The capital city of France is Paris.\n",
            "------------------------------------------\n",
            "quant: \n",
            "The capital city of France is a city in the French province of Brittany, France. It is located in the heart of the city of Brittany, and is the capital of the French province of Brittany. It is the largest city in Brittany, and the largest city in France. It is the largest city in France, and the largest city in France. It is the largest city in France, and the largest city in France. It is the largest city in France, and the largest city in France. It is the largest city in France, and the largest city in France. It is the largest city in France, and the largest city in France. It is the largest city\n",
            "generate 137 tokens using  3.3527s, 40.86207700111908 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Classify the following sentence: She quickly ran away.\n",
            "Input:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: This sentence is classified as an action.\n",
            "------------------------------------------\n",
            "quant: She quickly ran away.\n",
            "Input:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Input:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Input:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She quickly ran away.\n",
            "Output:\n",
            "She\n",
            "generate 137 tokens using  3.3479s, 40.92134454423043 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Edit the given text so that it uses formal language.\n",
            "Input:\n",
            "hey everyone, we need to finish up the project before the weekend\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Greetings everyone, we need to complete the project before the weekend.\n",
            "------------------------------------------\n",
            "quant: \n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a lot of work on this project.\n",
            "\n",
            "I'm going to do a\n",
            "generate 143 tokens using  4.1489s, 34.466596502055665 tokens/s.\n",
            "==========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TextGenerationPipeline\n",
        "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
        ")\n",
        "\n",
        "pretrained_model_dir = \"facebook/opt-125m\"\n",
        "quantized_model_dir = \"opt-125m-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)\n",
        "examples = [\n",
        "    tokenizer(\n",
        "        \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "quantize_config = BaseQuantizeConfig(\n",
        "    bits=4,  # quantize model to 4-bit\n",
        "    group_size=128,  # it is recommended to set the value to 128\n",
        "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
        ")\n",
        "\n",
        "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
        "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n",
        "\n",
        "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
        "model.quantize(examples)\n",
        "\n",
        "# save quantized model\n",
        "model.save_quantized(quantized_model_dir)\n",
        "\n",
        "# save quantized model using safetensors\n",
        "model.save_quantized(quantized_model_dir, use_safetensors=True)\n",
        "\n",
        "# push quantized model to Hugging Face Hub.\n",
        "# to use use_auth_token=True, Login first via huggingface-cli login.\n",
        "# or pass explcit token with: use_auth_token=\"hf_xxxxxxx\"\n",
        "# (uncomment the following three lines to enable this feature)\n",
        "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
        "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
        "# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)\n",
        "\n",
        "# alternatively you can save and push at the same time\n",
        "# (uncomment the following three lines to enable this feature)\n",
        "# repo_id = f\"YourUserName/{quantized_model_dir}\"\n",
        "# commit_message = f\"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}\"\n",
        "# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)\n",
        "\n",
        "# load quantized model to the first GPU\n",
        "model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device=\"cuda:0\")\n",
        "\n",
        "# download quantized model from Hugging Face Hub and load to the first GPU\n",
        "# model = AutoGPTQForCausalLM.from_quantized(repo_id, device=\"cuda:0\", use_safetensors=True, use_triton=False)\n",
        "\n",
        "# inference with model.generate\n",
        "print(tokenizer.decode(model.generate(**tokenizer(\"auto_gptq is\", return_tensors=\"pt\").to(model.device))[0]))\n",
        "\n",
        "# or you can also use pipeline\n",
        "pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
        "print(pipeline(\"auto-gptq is\")[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "b0mraAN684XH",
        "outputId": "7e4ed472-7b91-432e-d736-6c4e300946e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'auto_gptq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3b131ccd4a35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextGenerationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mauto_gptq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseQuantizeConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m logging.basicConfig(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auto_gptq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMJ2T0Zn84yI",
        "outputId": "a6472ee7-acc0-422e-8d0a-0fe1cf43f09e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (1.5.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.2.4)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.50.0)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto-gptq) (4.67.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->auto-gptq) (3.11.14)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto-gptq) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
            "Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge, gekko, auto-gptq\n",
            "Successfully installed auto-gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gD-umqyT89Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token XXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzAUdnyw9yOg",
        "outputId": "f3b21ec2-3dde-4423-82a8-04efdb1dee0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/quantization\n",
        "!python quant_with_alpaca.py --pretrained_model_dir \"meta-llama/Llama-3.2-1B\" --per_gpu_max_memory 1 --quant_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETBoVTud9i9n",
        "outputId": "6bb8aae7-e023-4a0f-cd80-f71856a8bcb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/quantization\n",
            "2025-03-26 05:41:47.430323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742967707.582837   15049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742967707.638613   15049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 05:41:47.777493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "2025-03-26 05:42:02 WARNING [accelerate.big_modeling] Some parameters are on the meta device because they were offloaded to the disk.\n",
            "Generating train split: 128 examples [00:00, 49078.61 examples/s]\n",
            "Map: 100% 128/128 [00:00<00:00, 2242.17 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/quantization/quant_with_alpaca.py\", line 216, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/quantization/quant_with_alpaca.py\", line 156, in main\n",
            "    model.quantize(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 382, in quantize\n",
            "    move_to_device(module, cur_layer_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_utils.py\", line 33, in move_to_device\n",
            "    obj = obj.to(device)\n",
            "          ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1336, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/quantization\n",
        "!python quant_with_alpaca.py --pretrained_model_dir \"Qwen/Qwen2.5-1.5B\" --per_gpu_max_memory 1 --quant_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cRiwsYR9ym3",
        "outputId": "51e113ce-a78e-46e6-8f7c-0ed89bba9aae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/quantization\n",
            "2025-03-26 05:45:13.450879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742967913.471320   15989 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742967913.477549   15989 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 05:45:13.498669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "tokenizer_config.json: 100% 7.23k/7.23k [00:00<00:00, 27.6MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 12.7MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 7.91MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 20.5MB/s]\n",
            "config.json: 100% 684/684 [00:00<00:00, 4.73MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "model.safetensors: 100% 3.09G/3.09G [00:17<00:00, 174MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 574kB/s]\n",
            "2025-03-26 05:45:50 WARNING [accelerate.big_modeling] Some parameters are on the meta device because they were offloaded to the disk.\n",
            "Generating train split: 128 examples [00:00, 4230.89 examples/s]\n",
            "Map: 100% 128/128 [00:00<00:00, 413.59 examples/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/quantization/quant_with_alpaca.py\", line 216, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/quantization/quant_with_alpaca.py\", line 156, in main\n",
            "    model.quantize(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 382, in quantize\n",
            "    move_to_device(module, cur_layer_device)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_utils.py\", line 33, in move_to_device\n",
            "    obj = obj.to(device)\n",
            "          ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1343, in to\n",
            "    return self._apply(convert)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
            "    param_applied = fn(param)\n",
            "                    ^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1336, in convert\n",
            "    raise NotImplementedError(\n",
            "NotImplementedError: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python /content/AutoGPTQ/examples/quantization/quant_with_alpaca.py --model_name_or_path Qwen/Qwen2.5-1.5B --bits 4 --trust_remote_code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn60QvgU_b2H",
        "outputId": "96ebffdb-889b-4c61-f92e-4e3dd95fdc6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 05:50:04.569383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742968204.591026   17241 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742968204.597196   17241 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 05:50:04.617301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "usage: quant_with_alpaca.py [-h] [--pretrained_model_dir PRETRAINED_MODEL_DIR]\n",
            "                            [--quantized_model_dir QUANTIZED_MODEL_DIR] [--bits {2,3,4,8}]\n",
            "                            [--group_size GROUP_SIZE] [--desc_act] [--num_samples NUM_SAMPLES]\n",
            "                            [--save_and_reload] [--fast_tokenizer] [--use_triton]\n",
            "                            [--per_gpu_max_memory PER_GPU_MAX_MEMORY]\n",
            "                            [--cpu_max_memory CPU_MAX_MEMORY]\n",
            "                            [--quant_batch_size QUANT_BATCH_SIZE] [--trust_remote_code]\n",
            "quant_with_alpaca.py: error: unrecognized arguments: --model_name_or_path Qwen/Qwen2.5-1.5B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python quant_with_alpaca.py --pretrained_model_dir \"Qwen/Qwen2.5-1.5B\" --per_gpu_max_memory 4 --quant_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_OoTZ-w_p3g",
        "outputId": "fdf87fee-3e9f-42ab-e3bc-a148881824eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-26 05:51:31.878312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742968291.898503   17597 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742968291.904616   17597 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 05:51:31.925164: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Generating train split: 128 examples [00:00, 45009.30 examples/s]\n",
            "Map: 100% 128/128 [00:00<00:00, 428.43 examples/s]\n",
            "INFO - Start quantizing layer 1/28\n",
            "2025-03-26 05:51:41 INFO [auto_gptq.modeling._base] Start quantizing layer 1/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 1/28...\n",
            "2025-03-26 05:51:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/28...\n",
            "2025-03-26 05:51:43 INFO [auto_gptq.quantization.gptq] duration: 1.2795486450195312\n",
            "2025-03-26 05:51:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.8471629619598389\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/28...\n",
            "2025-03-26 05:51:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/28...\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.quantization.gptq] duration: 0.5944533348083496\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.08846797794103622\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/28...\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/28...\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.quantization.gptq] duration: 0.6286811828613281\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.quantization.gptq] avg loss: 6.7993011474609375\n",
            "INFO - Quantizing self_attn.o_proj in layer 1/28...\n",
            "2025-03-26 05:51:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/28...\n",
            "2025-03-26 05:51:45 INFO [auto_gptq.quantization.gptq] duration: 0.7644848823547363\n",
            "2025-03-26 05:51:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.24179241061210632\n",
            "INFO - Quantizing mlp.up_proj in layer 1/28...\n",
            "2025-03-26 05:51:45 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/28...\n",
            "2025-03-26 05:51:46 INFO [auto_gptq.quantization.gptq] duration: 0.7628011703491211\n",
            "2025-03-26 05:51:46 INFO [auto_gptq.quantization.gptq] avg loss: 2.506913423538208\n",
            "INFO - Quantizing mlp.gate_proj in layer 1/28...\n",
            "2025-03-26 05:51:46 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/28...\n",
            "2025-03-26 05:51:47 INFO [auto_gptq.quantization.gptq] duration: 0.6217131614685059\n",
            "2025-03-26 05:51:47 INFO [auto_gptq.quantization.gptq] avg loss: 3.6670148372650146\n",
            "INFO - Quantizing mlp.down_proj in layer 1/28...\n",
            "2025-03-26 05:51:47 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/28...\n",
            "2025-03-26 05:51:52 INFO [auto_gptq.quantization.gptq] duration: 5.133577346801758\n",
            "2025-03-26 05:51:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.3388370871543884\n",
            "INFO - Start quantizing layer 2/28\n",
            "2025-03-26 05:51:52 INFO [auto_gptq.modeling._base] Start quantizing layer 2/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/28...\n",
            "2025-03-26 05:51:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/28...\n",
            "2025-03-26 05:51:53 INFO [auto_gptq.quantization.gptq] duration: 0.9649102687835693\n",
            "2025-03-26 05:51:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.306156724691391\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/28...\n",
            "2025-03-26 05:51:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/28...\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.quantization.gptq] duration: 0.8091607093811035\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.06981725245714188\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/28...\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/28...\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.quantization.gptq] duration: 0.5806388854980469\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.quantization.gptq] avg loss: 1.0424909591674805\n",
            "INFO - Quantizing self_attn.o_proj in layer 2/28...\n",
            "2025-03-26 05:51:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/28...\n",
            "2025-03-26 05:51:55 INFO [auto_gptq.quantization.gptq] duration: 0.7373020648956299\n",
            "2025-03-26 05:51:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.11305245012044907\n",
            "INFO - Quantizing mlp.up_proj in layer 2/28...\n",
            "2025-03-26 05:51:55 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/28...\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.quantization.gptq] duration: 0.7821919918060303\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.quantization.gptq] avg loss: 61.856441497802734\n",
            "INFO - Quantizing mlp.gate_proj in layer 2/28...\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/28...\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.quantization.gptq] duration: 0.5951333045959473\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.quantization.gptq] avg loss: 75.81534576416016\n",
            "INFO - Quantizing mlp.down_proj in layer 2/28...\n",
            "2025-03-26 05:51:56 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/28...\n",
            "2025-03-26 05:52:01 INFO [auto_gptq.quantization.gptq] duration: 5.019747495651245\n",
            "2025-03-26 05:52:01 INFO [auto_gptq.quantization.gptq] avg loss: 55.849971771240234\n",
            "INFO - Start quantizing layer 3/28\n",
            "2025-03-26 05:52:02 INFO [auto_gptq.modeling._base] Start quantizing layer 3/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/28...\n",
            "2025-03-26 05:52:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/28...\n",
            "2025-03-26 05:52:02 INFO [auto_gptq.quantization.gptq] duration: 0.7815179824829102\n",
            "2025-03-26 05:52:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.6450195908546448\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/28...\n",
            "2025-03-26 05:52:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/28...\n",
            "2025-03-26 05:52:03 INFO [auto_gptq.quantization.gptq] duration: 0.5912027359008789\n",
            "2025-03-26 05:52:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.2683746814727783\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/28...\n",
            "2025-03-26 05:52:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/28...\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.quantization.gptq] duration: 0.6389732360839844\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.quantization.gptq] avg loss: 2.9811127185821533\n",
            "INFO - Quantizing self_attn.o_proj in layer 3/28...\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/28...\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.quantization.gptq] duration: 0.8965816497802734\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.14112339913845062\n",
            "INFO - Quantizing mlp.up_proj in layer 3/28...\n",
            "2025-03-26 05:52:04 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/28...\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.quantization.gptq] duration: 1.0947892665863037\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.quantization.gptq] avg loss: 46.00367736816406\n",
            "INFO - Quantizing mlp.gate_proj in layer 3/28...\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/28...\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.quantization.gptq] duration: 0.6331131458282471\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.quantization.gptq] avg loss: 71.27751159667969\n",
            "INFO - Quantizing mlp.down_proj in layer 3/28...\n",
            "2025-03-26 05:52:06 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/28...\n",
            "2025-03-26 05:52:11 INFO [auto_gptq.quantization.gptq] duration: 5.023714065551758\n",
            "2025-03-26 05:52:11 INFO [auto_gptq.quantization.gptq] avg loss: 347.60723876953125\n",
            "INFO - Start quantizing layer 4/28\n",
            "2025-03-26 05:52:11 INFO [auto_gptq.modeling._base] Start quantizing layer 4/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/28...\n",
            "2025-03-26 05:52:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/28...\n",
            "2025-03-26 05:52:12 INFO [auto_gptq.quantization.gptq] duration: 0.786454439163208\n",
            "2025-03-26 05:52:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.6485724449157715\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/28...\n",
            "2025-03-26 05:52:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/28...\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.quantization.gptq] duration: 0.584709644317627\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.36172351241111755\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/28...\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/28...\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.quantization.gptq] duration: 0.5965852737426758\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.quantization.gptq] avg loss: 2.9994754791259766\n",
            "INFO - Quantizing self_attn.o_proj in layer 4/28...\n",
            "2025-03-26 05:52:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/28...\n",
            "2025-03-26 05:52:14 INFO [auto_gptq.quantization.gptq] duration: 0.7226712703704834\n",
            "2025-03-26 05:52:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.20482629537582397\n",
            "INFO - Quantizing mlp.up_proj in layer 4/28...\n",
            "2025-03-26 05:52:14 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/28...\n",
            "2025-03-26 05:52:15 INFO [auto_gptq.quantization.gptq] duration: 0.7651345729827881\n",
            "2025-03-26 05:52:15 INFO [auto_gptq.quantization.gptq] avg loss: 32.56328582763672\n",
            "INFO - Quantizing mlp.gate_proj in layer 4/28...\n",
            "2025-03-26 05:52:15 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/28...\n",
            "2025-03-26 05:52:16 INFO [auto_gptq.quantization.gptq] duration: 0.6199562549591064\n",
            "2025-03-26 05:52:16 INFO [auto_gptq.quantization.gptq] avg loss: 61.69731521606445\n",
            "INFO - Quantizing mlp.down_proj in layer 4/28...\n",
            "2025-03-26 05:52:16 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/28...\n",
            "2025-03-26 05:52:21 INFO [auto_gptq.quantization.gptq] duration: 5.205925464630127\n",
            "2025-03-26 05:52:21 INFO [auto_gptq.quantization.gptq] avg loss: 2.2143712043762207\n",
            "INFO - Start quantizing layer 5/28\n",
            "2025-03-26 05:52:21 INFO [auto_gptq.modeling._base] Start quantizing layer 5/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/28...\n",
            "2025-03-26 05:52:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/28...\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.quantization.gptq] duration: 0.787437915802002\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.746364951133728\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/28...\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/28...\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.quantization.gptq] duration: 0.5907461643218994\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.5040124654769897\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/28...\n",
            "2025-03-26 05:52:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/28...\n",
            "2025-03-26 05:52:23 INFO [auto_gptq.quantization.gptq] duration: 0.595238447189331\n",
            "2025-03-26 05:52:23 INFO [auto_gptq.quantization.gptq] avg loss: 3.856905460357666\n",
            "INFO - Quantizing self_attn.o_proj in layer 5/28...\n",
            "2025-03-26 05:52:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/28...\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.quantization.gptq] duration: 0.7395017147064209\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.2631285786628723\n",
            "INFO - Quantizing mlp.up_proj in layer 5/28...\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/28...\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.quantization.gptq] duration: 0.768380880355835\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.quantization.gptq] avg loss: 21.823945999145508\n",
            "INFO - Quantizing mlp.gate_proj in layer 5/28...\n",
            "2025-03-26 05:52:24 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/28...\n",
            "2025-03-26 05:52:25 INFO [auto_gptq.quantization.gptq] duration: 0.6087734699249268\n",
            "2025-03-26 05:52:25 INFO [auto_gptq.quantization.gptq] avg loss: 36.49485778808594\n",
            "INFO - Quantizing mlp.down_proj in layer 5/28...\n",
            "2025-03-26 05:52:25 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/28...\n",
            "2025-03-26 05:52:31 INFO [auto_gptq.quantization.gptq] duration: 5.67474365234375\n",
            "2025-03-26 05:52:31 INFO [auto_gptq.quantization.gptq] avg loss: 1.3701527118682861\n",
            "INFO - Start quantizing layer 6/28\n",
            "2025-03-26 05:52:31 INFO [auto_gptq.modeling._base] Start quantizing layer 6/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/28...\n",
            "2025-03-26 05:52:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/28...\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.quantization.gptq] duration: 0.8071291446685791\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.8512639403343201\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/28...\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/28...\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.quantization.gptq] duration: 0.5795726776123047\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.6860610246658325\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/28...\n",
            "2025-03-26 05:52:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/28...\n",
            "2025-03-26 05:52:33 INFO [auto_gptq.quantization.gptq] duration: 0.597602128982544\n",
            "2025-03-26 05:52:33 INFO [auto_gptq.quantization.gptq] avg loss: 3.9791693687438965\n",
            "INFO - Quantizing self_attn.o_proj in layer 6/28...\n",
            "2025-03-26 05:52:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/28...\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.quantization.gptq] duration: 0.7236297130584717\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.35202038288116455\n",
            "INFO - Quantizing mlp.up_proj in layer 6/28...\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/28...\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.quantization.gptq] duration: 0.760338544845581\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.quantization.gptq] avg loss: 44.390716552734375\n",
            "INFO - Quantizing mlp.gate_proj in layer 6/28...\n",
            "2025-03-26 05:52:34 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/28...\n",
            "2025-03-26 05:52:35 INFO [auto_gptq.quantization.gptq] duration: 0.6267762184143066\n",
            "2025-03-26 05:52:35 INFO [auto_gptq.quantization.gptq] avg loss: 68.580322265625\n",
            "INFO - Quantizing mlp.down_proj in layer 6/28...\n",
            "2025-03-26 05:52:35 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/28...\n",
            "2025-03-26 05:52:40 INFO [auto_gptq.quantization.gptq] duration: 5.253564357757568\n",
            "2025-03-26 05:52:40 INFO [auto_gptq.quantization.gptq] avg loss: 4.925701141357422\n",
            "INFO - Start quantizing layer 7/28\n",
            "2025-03-26 05:52:41 INFO [auto_gptq.modeling._base] Start quantizing layer 7/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/28...\n",
            "2025-03-26 05:52:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/28...\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.quantization.gptq] duration: 1.112253189086914\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.quantization.gptq] avg loss: 1.0715277194976807\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/28...\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/28...\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.quantization.gptq] duration: 0.645097017288208\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.5599799752235413\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/28...\n",
            "2025-03-26 05:52:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/28...\n",
            "2025-03-26 05:52:43 INFO [auto_gptq.quantization.gptq] duration: 0.6121158599853516\n",
            "2025-03-26 05:52:43 INFO [auto_gptq.quantization.gptq] avg loss: 5.1193437576293945\n",
            "INFO - Quantizing self_attn.o_proj in layer 7/28...\n",
            "2025-03-26 05:52:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/28...\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.quantization.gptq] duration: 0.7347631454467773\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.18207216262817383\n",
            "INFO - Quantizing mlp.up_proj in layer 7/28...\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/28...\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.quantization.gptq] duration: 0.7744085788726807\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.quantization.gptq] avg loss: 13.859508514404297\n",
            "INFO - Quantizing mlp.gate_proj in layer 7/28...\n",
            "2025-03-26 05:52:44 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/28...\n",
            "2025-03-26 05:52:45 INFO [auto_gptq.quantization.gptq] duration: 0.6150236129760742\n",
            "2025-03-26 05:52:45 INFO [auto_gptq.quantization.gptq] avg loss: 16.554828643798828\n",
            "INFO - Quantizing mlp.down_proj in layer 7/28...\n",
            "2025-03-26 05:52:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/28...\n",
            "2025-03-26 05:52:50 INFO [auto_gptq.quantization.gptq] duration: 5.086994647979736\n",
            "2025-03-26 05:52:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.7939091920852661\n",
            "INFO - Start quantizing layer 8/28\n",
            "2025-03-26 05:52:50 INFO [auto_gptq.modeling._base] Start quantizing layer 8/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/28...\n",
            "2025-03-26 05:52:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/28...\n",
            "2025-03-26 05:52:51 INFO [auto_gptq.quantization.gptq] duration: 0.7888121604919434\n",
            "2025-03-26 05:52:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.4643198251724243\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/28...\n",
            "2025-03-26 05:52:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/28...\n",
            "2025-03-26 05:52:52 INFO [auto_gptq.quantization.gptq] duration: 0.5937149524688721\n",
            "2025-03-26 05:52:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.36243313550949097\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/28...\n",
            "2025-03-26 05:52:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/28...\n",
            "2025-03-26 05:52:53 INFO [auto_gptq.quantization.gptq] duration: 0.8097081184387207\n",
            "2025-03-26 05:52:53 INFO [auto_gptq.quantization.gptq] avg loss: 2.3542592525482178\n",
            "INFO - Quantizing self_attn.o_proj in layer 8/28...\n",
            "2025-03-26 05:52:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/28...\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.quantization.gptq] duration: 1.031346321105957\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.22683778405189514\n",
            "INFO - Quantizing mlp.up_proj in layer 8/28...\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/28...\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.quantization.gptq] duration: 0.7987608909606934\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.quantization.gptq] avg loss: 13.076327323913574\n",
            "INFO - Quantizing mlp.gate_proj in layer 8/28...\n",
            "2025-03-26 05:52:54 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/28...\n",
            "2025-03-26 05:52:55 INFO [auto_gptq.quantization.gptq] duration: 0.6209180355072021\n",
            "2025-03-26 05:52:55 INFO [auto_gptq.quantization.gptq] avg loss: 13.693958282470703\n",
            "INFO - Quantizing mlp.down_proj in layer 8/28...\n",
            "2025-03-26 05:52:55 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/28...\n",
            "2025-03-26 05:53:01 INFO [auto_gptq.quantization.gptq] duration: 5.568345069885254\n",
            "2025-03-26 05:53:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.8279799818992615\n",
            "INFO - Start quantizing layer 9/28\n",
            "2025-03-26 05:53:01 INFO [auto_gptq.modeling._base] Start quantizing layer 9/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/28...\n",
            "2025-03-26 05:53:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/28...\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.quantization.gptq] duration: 0.790980339050293\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.7988307476043701\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/28...\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/28...\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.quantization.gptq] duration: 0.5814011096954346\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.49142611026763916\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/28...\n",
            "2025-03-26 05:53:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/28...\n",
            "2025-03-26 05:53:03 INFO [auto_gptq.quantization.gptq] duration: 0.6078102588653564\n",
            "2025-03-26 05:53:03 INFO [auto_gptq.quantization.gptq] avg loss: 4.451763153076172\n",
            "INFO - Quantizing self_attn.o_proj in layer 9/28...\n",
            "2025-03-26 05:53:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/28...\n",
            "2025-03-26 05:53:04 INFO [auto_gptq.quantization.gptq] duration: 0.7520856857299805\n",
            "2025-03-26 05:53:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.30731290578842163\n",
            "INFO - Quantizing mlp.up_proj in layer 9/28...\n",
            "2025-03-26 05:53:04 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/28...\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.quantization.gptq] duration: 0.9597811698913574\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.quantization.gptq] avg loss: 12.71929931640625\n",
            "INFO - Quantizing mlp.gate_proj in layer 9/28...\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/28...\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.quantization.gptq] duration: 0.810265064239502\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.quantization.gptq] avg loss: 13.868253707885742\n",
            "INFO - Quantizing mlp.down_proj in layer 9/28...\n",
            "2025-03-26 05:53:05 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/28...\n",
            "2025-03-26 05:53:10 INFO [auto_gptq.quantization.gptq] duration: 5.017487049102783\n",
            "2025-03-26 05:53:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.7241165637969971\n",
            "INFO - Start quantizing layer 10/28\n",
            "2025-03-26 05:53:11 INFO [auto_gptq.modeling._base] Start quantizing layer 10/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/28...\n",
            "2025-03-26 05:53:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/28...\n",
            "2025-03-26 05:53:11 INFO [auto_gptq.quantization.gptq] duration: 0.8094568252563477\n",
            "2025-03-26 05:53:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.71950364112854\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/28...\n",
            "2025-03-26 05:53:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/28...\n",
            "2025-03-26 05:53:12 INFO [auto_gptq.quantization.gptq] duration: 0.5995712280273438\n",
            "2025-03-26 05:53:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.4082232117652893\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/28...\n",
            "2025-03-26 05:53:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/28...\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.quantization.gptq] duration: 0.5934906005859375\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.quantization.gptq] avg loss: 3.469752788543701\n",
            "INFO - Quantizing self_attn.o_proj in layer 10/28...\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/28...\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.quantization.gptq] duration: 0.7447018623352051\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.28505802154541016\n",
            "INFO - Quantizing mlp.up_proj in layer 10/28...\n",
            "2025-03-26 05:53:13 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/28...\n",
            "2025-03-26 05:53:14 INFO [auto_gptq.quantization.gptq] duration: 0.7793452739715576\n",
            "2025-03-26 05:53:14 INFO [auto_gptq.quantization.gptq] avg loss: 12.04066276550293\n",
            "INFO - Quantizing mlp.gate_proj in layer 10/28...\n",
            "2025-03-26 05:53:14 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/28...\n",
            "2025-03-26 05:53:15 INFO [auto_gptq.quantization.gptq] duration: 0.6122622489929199\n",
            "2025-03-26 05:53:15 INFO [auto_gptq.quantization.gptq] avg loss: 12.458104133605957\n",
            "INFO - Quantizing mlp.down_proj in layer 10/28...\n",
            "2025-03-26 05:53:15 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/28...\n",
            "2025-03-26 05:53:20 INFO [auto_gptq.quantization.gptq] duration: 5.549876689910889\n",
            "2025-03-26 05:53:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.7083075046539307\n",
            "INFO - Start quantizing layer 11/28\n",
            "2025-03-26 05:53:20 INFO [auto_gptq.modeling._base] Start quantizing layer 11/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/28...\n",
            "2025-03-26 05:53:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/28...\n",
            "2025-03-26 05:53:21 INFO [auto_gptq.quantization.gptq] duration: 0.7870008945465088\n",
            "2025-03-26 05:53:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.6661278009414673\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/28...\n",
            "2025-03-26 05:53:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/28...\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.quantization.gptq] duration: 0.5875551700592041\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.5668715238571167\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/28...\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/28...\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.quantization.gptq] duration: 0.6133849620819092\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.quantization.gptq] avg loss: 3.366966724395752\n",
            "INFO - Quantizing self_attn.o_proj in layer 11/28...\n",
            "2025-03-26 05:53:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/28...\n",
            "2025-03-26 05:53:23 INFO [auto_gptq.quantization.gptq] duration: 0.7323577404022217\n",
            "2025-03-26 05:53:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.4970666468143463\n",
            "INFO - Quantizing mlp.up_proj in layer 11/28...\n",
            "2025-03-26 05:53:23 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/28...\n",
            "2025-03-26 05:53:24 INFO [auto_gptq.quantization.gptq] duration: 0.7523343563079834\n",
            "2025-03-26 05:53:24 INFO [auto_gptq.quantization.gptq] avg loss: 11.586448669433594\n",
            "INFO - Quantizing mlp.gate_proj in layer 11/28...\n",
            "2025-03-26 05:53:24 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/28...\n",
            "2025-03-26 05:53:25 INFO [auto_gptq.quantization.gptq] duration: 0.6172869205474854\n",
            "2025-03-26 05:53:25 INFO [auto_gptq.quantization.gptq] avg loss: 12.341681480407715\n",
            "INFO - Quantizing mlp.down_proj in layer 11/28...\n",
            "2025-03-26 05:53:25 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/28...\n",
            "2025-03-26 05:53:30 INFO [auto_gptq.quantization.gptq] duration: 5.716892719268799\n",
            "2025-03-26 05:53:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.6951650381088257\n",
            "INFO - Start quantizing layer 12/28\n",
            "2025-03-26 05:53:31 INFO [auto_gptq.modeling._base] Start quantizing layer 12/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/28...\n",
            "2025-03-26 05:53:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/28...\n",
            "2025-03-26 05:53:31 INFO [auto_gptq.quantization.gptq] duration: 0.8020951747894287\n",
            "2025-03-26 05:53:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.6882318258285522\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/28...\n",
            "2025-03-26 05:53:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/28...\n",
            "2025-03-26 05:53:32 INFO [auto_gptq.quantization.gptq] duration: 0.5828497409820557\n",
            "2025-03-26 05:53:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.48835456371307373\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/28...\n",
            "2025-03-26 05:53:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/28...\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.quantization.gptq] duration: 0.6016063690185547\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.quantization.gptq] avg loss: 3.479912042617798\n",
            "INFO - Quantizing self_attn.o_proj in layer 12/28...\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/28...\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.quantization.gptq] duration: 0.726078987121582\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.4603673219680786\n",
            "INFO - Quantizing mlp.up_proj in layer 12/28...\n",
            "2025-03-26 05:53:33 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/28...\n",
            "2025-03-26 05:53:34 INFO [auto_gptq.quantization.gptq] duration: 0.7580313682556152\n",
            "2025-03-26 05:53:34 INFO [auto_gptq.quantization.gptq] avg loss: 10.713987350463867\n",
            "INFO - Quantizing mlp.gate_proj in layer 12/28...\n",
            "2025-03-26 05:53:34 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/28...\n",
            "2025-03-26 05:53:35 INFO [auto_gptq.quantization.gptq] duration: 0.6273238658905029\n",
            "2025-03-26 05:53:35 INFO [auto_gptq.quantization.gptq] avg loss: 12.2451171875\n",
            "INFO - Quantizing mlp.down_proj in layer 12/28...\n",
            "2025-03-26 05:53:35 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/28...\n",
            "2025-03-26 05:53:40 INFO [auto_gptq.quantization.gptq] duration: 5.1048407554626465\n",
            "2025-03-26 05:53:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.5657109022140503\n",
            "INFO - Start quantizing layer 13/28\n",
            "2025-03-26 05:53:40 INFO [auto_gptq.modeling._base] Start quantizing layer 13/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 13/28...\n",
            "2025-03-26 05:53:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/28...\n",
            "2025-03-26 05:53:41 INFO [auto_gptq.quantization.gptq] duration: 0.9574637413024902\n",
            "2025-03-26 05:53:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.8595946431159973\n",
            "INFO - Quantizing self_attn.v_proj in layer 13/28...\n",
            "2025-03-26 05:53:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/28...\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.quantization.gptq] duration: 0.8728792667388916\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.4486452341079712\n",
            "INFO - Quantizing self_attn.q_proj in layer 13/28...\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/28...\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.quantization.gptq] duration: 0.6258413791656494\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.quantization.gptq] avg loss: 3.997283458709717\n",
            "INFO - Quantizing self_attn.o_proj in layer 13/28...\n",
            "2025-03-26 05:53:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/28...\n",
            "2025-03-26 05:53:43 INFO [auto_gptq.quantization.gptq] duration: 0.745159387588501\n",
            "2025-03-26 05:53:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.2951482832431793\n",
            "INFO - Quantizing mlp.up_proj in layer 13/28...\n",
            "2025-03-26 05:53:43 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/28...\n",
            "2025-03-26 05:53:44 INFO [auto_gptq.quantization.gptq] duration: 0.7665631771087646\n",
            "2025-03-26 05:53:44 INFO [auto_gptq.quantization.gptq] avg loss: 10.265005111694336\n",
            "INFO - Quantizing mlp.gate_proj in layer 13/28...\n",
            "2025-03-26 05:53:44 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/28...\n",
            "2025-03-26 05:53:45 INFO [auto_gptq.quantization.gptq] duration: 0.5967659950256348\n",
            "2025-03-26 05:53:45 INFO [auto_gptq.quantization.gptq] avg loss: 11.337677955627441\n",
            "INFO - Quantizing mlp.down_proj in layer 13/28...\n",
            "2025-03-26 05:53:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/28...\n",
            "2025-03-26 05:53:50 INFO [auto_gptq.quantization.gptq] duration: 5.166356086730957\n",
            "2025-03-26 05:53:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.6216731071472168\n",
            "INFO - Start quantizing layer 14/28\n",
            "2025-03-26 05:53:50 INFO [auto_gptq.modeling._base] Start quantizing layer 14/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 14/28...\n",
            "2025-03-26 05:53:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/28...\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.quantization.gptq] duration: 0.7927219867706299\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.5657510757446289\n",
            "INFO - Quantizing self_attn.v_proj in layer 14/28...\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/28...\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.quantization.gptq] duration: 0.5912325382232666\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.3764982223510742\n",
            "INFO - Quantizing self_attn.q_proj in layer 14/28...\n",
            "2025-03-26 05:53:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/28...\n",
            "2025-03-26 05:53:52 INFO [auto_gptq.quantization.gptq] duration: 0.6229867935180664\n",
            "2025-03-26 05:53:52 INFO [auto_gptq.quantization.gptq] avg loss: 2.8059301376342773\n",
            "INFO - Quantizing self_attn.o_proj in layer 14/28...\n",
            "2025-03-26 05:53:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/28...\n",
            "2025-03-26 05:53:53 INFO [auto_gptq.quantization.gptq] duration: 0.9265542030334473\n",
            "2025-03-26 05:53:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.3507664203643799\n",
            "INFO - Quantizing mlp.up_proj in layer 14/28...\n",
            "2025-03-26 05:53:53 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/28...\n",
            "2025-03-26 05:53:54 INFO [auto_gptq.quantization.gptq] duration: 1.0587706565856934\n",
            "2025-03-26 05:53:54 INFO [auto_gptq.quantization.gptq] avg loss: 9.922781944274902\n",
            "INFO - Quantizing mlp.gate_proj in layer 14/28...\n",
            "2025-03-26 05:53:54 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/28...\n",
            "2025-03-26 05:53:55 INFO [auto_gptq.quantization.gptq] duration: 0.628328800201416\n",
            "2025-03-26 05:53:55 INFO [auto_gptq.quantization.gptq] avg loss: 10.349295616149902\n",
            "INFO - Quantizing mlp.down_proj in layer 14/28...\n",
            "2025-03-26 05:53:55 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/28...\n",
            "2025-03-26 05:54:00 INFO [auto_gptq.quantization.gptq] duration: 5.197998285293579\n",
            "2025-03-26 05:54:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.5149750709533691\n",
            "INFO - Start quantizing layer 15/28\n",
            "2025-03-26 05:54:00 INFO [auto_gptq.modeling._base] Start quantizing layer 15/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 15/28...\n",
            "2025-03-26 05:54:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/28...\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.quantization.gptq] duration: 0.7833893299102783\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.9578772187232971\n",
            "INFO - Quantizing self_attn.v_proj in layer 15/28...\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/28...\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.quantization.gptq] duration: 0.5889649391174316\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.7133693099021912\n",
            "INFO - Quantizing self_attn.q_proj in layer 15/28...\n",
            "2025-03-26 05:54:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/28...\n",
            "2025-03-26 05:54:02 INFO [auto_gptq.quantization.gptq] duration: 0.5898776054382324\n",
            "2025-03-26 05:54:02 INFO [auto_gptq.quantization.gptq] avg loss: 6.327810764312744\n",
            "INFO - Quantizing self_attn.o_proj in layer 15/28...\n",
            "2025-03-26 05:54:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/28...\n",
            "2025-03-26 05:54:03 INFO [auto_gptq.quantization.gptq] duration: 0.7560915946960449\n",
            "2025-03-26 05:54:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.3199999928474426\n",
            "INFO - Quantizing mlp.up_proj in layer 15/28...\n",
            "2025-03-26 05:54:03 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/28...\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.quantization.gptq] duration: 0.7752995491027832\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.quantization.gptq] avg loss: 11.190269470214844\n",
            "INFO - Quantizing mlp.gate_proj in layer 15/28...\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/28...\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.quantization.gptq] duration: 0.7277534008026123\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.quantization.gptq] avg loss: 11.028255462646484\n",
            "INFO - Quantizing mlp.down_proj in layer 15/28...\n",
            "2025-03-26 05:54:04 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/28...\n",
            "2025-03-26 05:54:09 INFO [auto_gptq.quantization.gptq] duration: 5.215210676193237\n",
            "2025-03-26 05:54:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.6286423206329346\n",
            "INFO - Start quantizing layer 16/28\n",
            "2025-03-26 05:54:10 INFO [auto_gptq.modeling._base] Start quantizing layer 16/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 16/28...\n",
            "2025-03-26 05:54:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/28...\n",
            "2025-03-26 05:54:10 INFO [auto_gptq.quantization.gptq] duration: 0.7982864379882812\n",
            "2025-03-26 05:54:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.6687473058700562\n",
            "INFO - Quantizing self_attn.v_proj in layer 16/28...\n",
            "2025-03-26 05:54:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/28...\n",
            "2025-03-26 05:54:11 INFO [auto_gptq.quantization.gptq] duration: 0.5960721969604492\n",
            "2025-03-26 05:54:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.5935686230659485\n",
            "INFO - Quantizing self_attn.q_proj in layer 16/28...\n",
            "2025-03-26 05:54:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/28...\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.quantization.gptq] duration: 0.6017258167266846\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.quantization.gptq] avg loss: 5.996422290802002\n",
            "INFO - Quantizing self_attn.o_proj in layer 16/28...\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/28...\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.quantization.gptq] duration: 0.7303462028503418\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.5208926200866699\n",
            "INFO - Quantizing mlp.up_proj in layer 16/28...\n",
            "2025-03-26 05:54:12 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/28...\n",
            "2025-03-26 05:54:13 INFO [auto_gptq.quantization.gptq] duration: 0.7765810489654541\n",
            "2025-03-26 05:54:13 INFO [auto_gptq.quantization.gptq] avg loss: 9.75417709350586\n",
            "INFO - Quantizing mlp.gate_proj in layer 16/28...\n",
            "2025-03-26 05:54:13 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/28...\n",
            "2025-03-26 05:54:14 INFO [auto_gptq.quantization.gptq] duration: 0.6196205615997314\n",
            "2025-03-26 05:54:14 INFO [auto_gptq.quantization.gptq] avg loss: 10.343219757080078\n",
            "INFO - Quantizing mlp.down_proj in layer 16/28...\n",
            "2025-03-26 05:54:14 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/28...\n",
            "2025-03-26 05:54:20 INFO [auto_gptq.quantization.gptq] duration: 5.809180021286011\n",
            "2025-03-26 05:54:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.5612668991088867\n",
            "INFO - Start quantizing layer 17/28\n",
            "2025-03-26 05:54:20 INFO [auto_gptq.modeling._base] Start quantizing layer 17/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 17/28...\n",
            "2025-03-26 05:54:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 17/28...\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.quantization.gptq] duration: 0.8009207248687744\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.8932945728302002\n",
            "INFO - Quantizing self_attn.v_proj in layer 17/28...\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 17/28...\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.quantization.gptq] duration: 0.5969235897064209\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.679503321647644\n",
            "INFO - Quantizing self_attn.q_proj in layer 17/28...\n",
            "2025-03-26 05:54:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 17/28...\n",
            "2025-03-26 05:54:22 INFO [auto_gptq.quantization.gptq] duration: 0.6036365032196045\n",
            "2025-03-26 05:54:22 INFO [auto_gptq.quantization.gptq] avg loss: 4.679037094116211\n",
            "INFO - Quantizing self_attn.o_proj in layer 17/28...\n",
            "2025-03-26 05:54:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 17/28...\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.quantization.gptq] duration: 0.7475025653839111\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.32807081937789917\n",
            "INFO - Quantizing mlp.up_proj in layer 17/28...\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 17/28...\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.quantization.gptq] duration: 0.7721302509307861\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.quantization.gptq] avg loss: 11.602450370788574\n",
            "INFO - Quantizing mlp.gate_proj in layer 17/28...\n",
            "2025-03-26 05:54:23 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 17/28...\n",
            "2025-03-26 05:54:24 INFO [auto_gptq.quantization.gptq] duration: 0.6239955425262451\n",
            "2025-03-26 05:54:24 INFO [auto_gptq.quantization.gptq] avg loss: 11.84902572631836\n",
            "INFO - Quantizing mlp.down_proj in layer 17/28...\n",
            "2025-03-26 05:54:24 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 17/28...\n",
            "2025-03-26 05:54:32 INFO [auto_gptq.quantization.gptq] duration: 7.914918422698975\n",
            "2025-03-26 05:54:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.7114168405532837\n",
            "INFO - Start quantizing layer 18/28\n",
            "2025-03-26 05:54:32 INFO [auto_gptq.modeling._base] Start quantizing layer 18/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 18/28...\n",
            "2025-03-26 05:54:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 18/28...\n",
            "2025-03-26 05:54:33 INFO [auto_gptq.quantization.gptq] duration: 1.1257586479187012\n",
            "2025-03-26 05:54:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.5354362726211548\n",
            "INFO - Quantizing self_attn.v_proj in layer 18/28...\n",
            "2025-03-26 05:54:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 18/28...\n",
            "2025-03-26 05:54:34 INFO [auto_gptq.quantization.gptq] duration: 0.8950371742248535\n",
            "2025-03-26 05:54:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.925754189491272\n",
            "INFO - Quantizing self_attn.q_proj in layer 18/28...\n",
            "2025-03-26 05:54:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 18/28...\n",
            "2025-03-26 05:54:35 INFO [auto_gptq.quantization.gptq] duration: 0.928478479385376\n",
            "2025-03-26 05:54:35 INFO [auto_gptq.quantization.gptq] avg loss: 4.2975263595581055\n",
            "INFO - Quantizing self_attn.o_proj in layer 18/28...\n",
            "2025-03-26 05:54:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 18/28...\n",
            "2025-03-26 05:54:36 INFO [auto_gptq.quantization.gptq] duration: 1.0932624340057373\n",
            "2025-03-26 05:54:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.3334871828556061\n",
            "INFO - Quantizing mlp.up_proj in layer 18/28...\n",
            "2025-03-26 05:54:36 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 18/28...\n",
            "2025-03-26 05:54:37 INFO [auto_gptq.quantization.gptq] duration: 1.0595500469207764\n",
            "2025-03-26 05:54:37 INFO [auto_gptq.quantization.gptq] avg loss: 12.509700775146484\n",
            "INFO - Quantizing mlp.gate_proj in layer 18/28...\n",
            "2025-03-26 05:54:37 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 18/28...\n",
            "2025-03-26 05:54:38 INFO [auto_gptq.quantization.gptq] duration: 0.9216909408569336\n",
            "2025-03-26 05:54:38 INFO [auto_gptq.quantization.gptq] avg loss: 12.620397567749023\n",
            "INFO - Quantizing mlp.down_proj in layer 18/28...\n",
            "2025-03-26 05:54:38 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 18/28...\n",
            "2025-03-26 05:54:44 INFO [auto_gptq.quantization.gptq] duration: 6.137075424194336\n",
            "2025-03-26 05:54:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.9740406274795532\n",
            "INFO - Start quantizing layer 19/28\n",
            "2025-03-26 05:54:45 INFO [auto_gptq.modeling._base] Start quantizing layer 19/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 19/28...\n",
            "2025-03-26 05:54:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 19/28...\n",
            "2025-03-26 05:54:45 INFO [auto_gptq.quantization.gptq] duration: 0.7961418628692627\n",
            "2025-03-26 05:54:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.6868968605995178\n",
            "INFO - Quantizing self_attn.v_proj in layer 19/28...\n",
            "2025-03-26 05:54:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 19/28...\n",
            "2025-03-26 05:54:46 INFO [auto_gptq.quantization.gptq] duration: 0.6094539165496826\n",
            "2025-03-26 05:54:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.6877952814102173\n",
            "INFO - Quantizing self_attn.q_proj in layer 19/28...\n",
            "2025-03-26 05:54:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 19/28...\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.quantization.gptq] duration: 0.6254913806915283\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.quantization.gptq] avg loss: 4.029690742492676\n",
            "INFO - Quantizing self_attn.o_proj in layer 19/28...\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 19/28...\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.quantization.gptq] duration: 0.7408745288848877\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.7827416658401489\n",
            "INFO - Quantizing mlp.up_proj in layer 19/28...\n",
            "2025-03-26 05:54:47 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 19/28...\n",
            "2025-03-26 05:54:48 INFO [auto_gptq.quantization.gptq] duration: 0.7947945594787598\n",
            "2025-03-26 05:54:48 INFO [auto_gptq.quantization.gptq] avg loss: 14.19198226928711\n",
            "INFO - Quantizing mlp.gate_proj in layer 19/28...\n",
            "2025-03-26 05:54:48 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 19/28...\n",
            "2025-03-26 05:54:49 INFO [auto_gptq.quantization.gptq] duration: 0.6219480037689209\n",
            "2025-03-26 05:54:49 INFO [auto_gptq.quantization.gptq] avg loss: 14.47364330291748\n",
            "INFO - Quantizing mlp.down_proj in layer 19/28...\n",
            "2025-03-26 05:54:49 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 19/28...\n",
            "2025-03-26 05:54:55 INFO [auto_gptq.quantization.gptq] duration: 5.821406364440918\n",
            "2025-03-26 05:54:55 INFO [auto_gptq.quantization.gptq] avg loss: 1.397733211517334\n",
            "INFO - Start quantizing layer 20/28\n",
            "2025-03-26 05:54:55 INFO [auto_gptq.modeling._base] Start quantizing layer 20/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 20/28...\n",
            "2025-03-26 05:54:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 20/28...\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.quantization.gptq] duration: 0.8087913990020752\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.7497323751449585\n",
            "INFO - Quantizing self_attn.v_proj in layer 20/28...\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 20/28...\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.quantization.gptq] duration: 0.6112861633300781\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.quantization.gptq] avg loss: 1.5128005743026733\n",
            "INFO - Quantizing self_attn.q_proj in layer 20/28...\n",
            "2025-03-26 05:54:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 20/28...\n",
            "2025-03-26 05:54:57 INFO [auto_gptq.quantization.gptq] duration: 0.6197614669799805\n",
            "2025-03-26 05:54:57 INFO [auto_gptq.quantization.gptq] avg loss: 5.650176048278809\n",
            "INFO - Quantizing self_attn.o_proj in layer 20/28...\n",
            "2025-03-26 05:54:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 20/28...\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.quantization.gptq] duration: 0.7930200099945068\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.8258205056190491\n",
            "INFO - Quantizing mlp.up_proj in layer 20/28...\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 20/28...\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.quantization.gptq] duration: 0.7955238819122314\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.quantization.gptq] avg loss: 17.999385833740234\n",
            "INFO - Quantizing mlp.gate_proj in layer 20/28...\n",
            "2025-03-26 05:54:58 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 20/28...\n",
            "2025-03-26 05:54:59 INFO [auto_gptq.quantization.gptq] duration: 0.6106085777282715\n",
            "2025-03-26 05:54:59 INFO [auto_gptq.quantization.gptq] avg loss: 16.835506439208984\n",
            "INFO - Quantizing mlp.down_proj in layer 20/28...\n",
            "2025-03-26 05:54:59 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 20/28...\n",
            "2025-03-26 05:55:04 INFO [auto_gptq.quantization.gptq] duration: 5.197269439697266\n",
            "2025-03-26 05:55:04 INFO [auto_gptq.quantization.gptq] avg loss: 2.563114643096924\n",
            "INFO - Start quantizing layer 21/28\n",
            "2025-03-26 05:55:04 INFO [auto_gptq.modeling._base] Start quantizing layer 21/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 21/28...\n",
            "2025-03-26 05:55:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 21/28...\n",
            "2025-03-26 05:55:05 INFO [auto_gptq.quantization.gptq] duration: 1.0159976482391357\n",
            "2025-03-26 05:55:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.8590396642684937\n",
            "INFO - Quantizing self_attn.v_proj in layer 21/28...\n",
            "2025-03-26 05:55:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 21/28...\n",
            "2025-03-26 05:55:06 INFO [auto_gptq.quantization.gptq] duration: 0.7887585163116455\n",
            "2025-03-26 05:55:06 INFO [auto_gptq.quantization.gptq] avg loss: 1.9825546741485596\n",
            "INFO - Quantizing self_attn.q_proj in layer 21/28...\n",
            "2025-03-26 05:55:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 21/28...\n",
            "2025-03-26 05:55:07 INFO [auto_gptq.quantization.gptq] duration: 0.8333489894866943\n",
            "2025-03-26 05:55:07 INFO [auto_gptq.quantization.gptq] avg loss: 6.6003923416137695\n",
            "INFO - Quantizing self_attn.o_proj in layer 21/28...\n",
            "2025-03-26 05:55:07 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 21/28...\n",
            "2025-03-26 05:55:08 INFO [auto_gptq.quantization.gptq] duration: 0.7803246974945068\n",
            "2025-03-26 05:55:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.1452542543411255\n",
            "INFO - Quantizing mlp.up_proj in layer 21/28...\n",
            "2025-03-26 05:55:08 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 21/28...\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.quantization.gptq] duration: 0.7857909202575684\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.quantization.gptq] avg loss: 19.05603790283203\n",
            "INFO - Quantizing mlp.gate_proj in layer 21/28...\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 21/28...\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.quantization.gptq] duration: 0.6118724346160889\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.quantization.gptq] avg loss: 18.64706039428711\n",
            "INFO - Quantizing mlp.down_proj in layer 21/28...\n",
            "2025-03-26 05:55:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 21/28...\n",
            "2025-03-26 05:55:15 INFO [auto_gptq.quantization.gptq] duration: 5.231227397918701\n",
            "2025-03-26 05:55:15 INFO [auto_gptq.quantization.gptq] avg loss: 2.316223621368408\n",
            "INFO - Start quantizing layer 22/28\n",
            "2025-03-26 05:55:15 INFO [auto_gptq.modeling._base] Start quantizing layer 22/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 22/28...\n",
            "2025-03-26 05:55:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 22/28...\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.quantization.gptq] duration: 0.8239448070526123\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.8167798519134521\n",
            "INFO - Quantizing self_attn.v_proj in layer 22/28...\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 22/28...\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.quantization.gptq] duration: 0.5876674652099609\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.quantization.gptq] avg loss: 1.7475091218948364\n",
            "INFO - Quantizing self_attn.q_proj in layer 22/28...\n",
            "2025-03-26 05:55:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 22/28...\n",
            "2025-03-26 05:55:17 INFO [auto_gptq.quantization.gptq] duration: 0.6052694320678711\n",
            "2025-03-26 05:55:17 INFO [auto_gptq.quantization.gptq] avg loss: 6.506231307983398\n",
            "INFO - Quantizing self_attn.o_proj in layer 22/28...\n",
            "2025-03-26 05:55:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 22/28...\n",
            "2025-03-26 05:55:18 INFO [auto_gptq.quantization.gptq] duration: 0.9988296031951904\n",
            "2025-03-26 05:55:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.7293756604194641\n",
            "INFO - Quantizing mlp.up_proj in layer 22/28...\n",
            "2025-03-26 05:55:18 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 22/28...\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.quantization.gptq] duration: 1.0726611614227295\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.quantization.gptq] avg loss: 25.999027252197266\n",
            "INFO - Quantizing mlp.gate_proj in layer 22/28...\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 22/28...\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.quantization.gptq] duration: 0.635444164276123\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.quantization.gptq] avg loss: 26.00904655456543\n",
            "INFO - Quantizing mlp.down_proj in layer 22/28...\n",
            "2025-03-26 05:55:19 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 22/28...\n",
            "2025-03-26 05:55:25 INFO [auto_gptq.quantization.gptq] duration: 5.2466490268707275\n",
            "2025-03-26 05:55:25 INFO [auto_gptq.quantization.gptq] avg loss: 4.0867180824279785\n",
            "INFO - Start quantizing layer 23/28\n",
            "2025-03-26 05:55:25 INFO [auto_gptq.modeling._base] Start quantizing layer 23/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 23/28...\n",
            "2025-03-26 05:55:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 23/28...\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.quantization.gptq] duration: 0.8002357482910156\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.97221440076828\n",
            "INFO - Quantizing self_attn.v_proj in layer 23/28...\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 23/28...\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.quantization.gptq] duration: 0.5866243839263916\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.quantization.gptq] avg loss: 2.172506809234619\n",
            "INFO - Quantizing self_attn.q_proj in layer 23/28...\n",
            "2025-03-26 05:55:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 23/28...\n",
            "2025-03-26 05:55:27 INFO [auto_gptq.quantization.gptq] duration: 0.6381716728210449\n",
            "2025-03-26 05:55:27 INFO [auto_gptq.quantization.gptq] avg loss: 6.355258464813232\n",
            "INFO - Quantizing self_attn.o_proj in layer 23/28...\n",
            "2025-03-26 05:55:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 23/28...\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.quantization.gptq] duration: 0.769202709197998\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.7619128227233887\n",
            "INFO - Quantizing mlp.up_proj in layer 23/28...\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 23/28...\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.quantization.gptq] duration: 0.7694470882415771\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.quantization.gptq] avg loss: 30.41119384765625\n",
            "INFO - Quantizing mlp.gate_proj in layer 23/28...\n",
            "2025-03-26 05:55:28 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 23/28...\n",
            "2025-03-26 05:55:29 INFO [auto_gptq.quantization.gptq] duration: 0.679929256439209\n",
            "2025-03-26 05:55:29 INFO [auto_gptq.quantization.gptq] avg loss: 30.77862548828125\n",
            "INFO - Quantizing mlp.down_proj in layer 23/28...\n",
            "2025-03-26 05:55:29 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 23/28...\n",
            "2025-03-26 05:55:34 INFO [auto_gptq.quantization.gptq] duration: 5.2533347606658936\n",
            "2025-03-26 05:55:34 INFO [auto_gptq.quantization.gptq] avg loss: 4.7586259841918945\n",
            "INFO - Start quantizing layer 24/28\n",
            "2025-03-26 05:55:35 INFO [auto_gptq.modeling._base] Start quantizing layer 24/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 24/28...\n",
            "2025-03-26 05:55:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 24/28...\n",
            "2025-03-26 05:55:35 INFO [auto_gptq.quantization.gptq] duration: 0.8337702751159668\n",
            "2025-03-26 05:55:35 INFO [auto_gptq.quantization.gptq] avg loss: 1.1434680223464966\n",
            "INFO - Quantizing self_attn.v_proj in layer 24/28...\n",
            "2025-03-26 05:55:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 24/28...\n",
            "2025-03-26 05:55:36 INFO [auto_gptq.quantization.gptq] duration: 0.620028018951416\n",
            "2025-03-26 05:55:36 INFO [auto_gptq.quantization.gptq] avg loss: 3.2734007835388184\n",
            "INFO - Quantizing self_attn.q_proj in layer 24/28...\n",
            "2025-03-26 05:55:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 24/28...\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.quantization.gptq] duration: 0.5944488048553467\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.quantization.gptq] avg loss: 9.615367889404297\n",
            "INFO - Quantizing self_attn.o_proj in layer 24/28...\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 24/28...\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.quantization.gptq] duration: 0.7530472278594971\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.9672690629959106\n",
            "INFO - Quantizing mlp.up_proj in layer 24/28...\n",
            "2025-03-26 05:55:37 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 24/28...\n",
            "2025-03-26 05:55:38 INFO [auto_gptq.quantization.gptq] duration: 0.7969439029693604\n",
            "2025-03-26 05:55:38 INFO [auto_gptq.quantization.gptq] avg loss: 31.347307205200195\n",
            "INFO - Quantizing mlp.gate_proj in layer 24/28...\n",
            "2025-03-26 05:55:38 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 24/28...\n",
            "2025-03-26 05:55:39 INFO [auto_gptq.quantization.gptq] duration: 0.6099894046783447\n",
            "2025-03-26 05:55:39 INFO [auto_gptq.quantization.gptq] avg loss: 30.17622184753418\n",
            "INFO - Quantizing mlp.down_proj in layer 24/28...\n",
            "2025-03-26 05:55:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 24/28...\n",
            "2025-03-26 05:55:45 INFO [auto_gptq.quantization.gptq] duration: 5.854510068893433\n",
            "2025-03-26 05:55:45 INFO [auto_gptq.quantization.gptq] avg loss: 4.844760894775391\n",
            "INFO - Start quantizing layer 25/28\n",
            "2025-03-26 05:55:45 INFO [auto_gptq.modeling._base] Start quantizing layer 25/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 25/28...\n",
            "2025-03-26 05:55:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 25/28...\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.quantization.gptq] duration: 0.8117814064025879\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.quantization.gptq] avg loss: 1.1113882064819336\n",
            "INFO - Quantizing self_attn.v_proj in layer 25/28...\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 25/28...\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.quantization.gptq] duration: 0.5758883953094482\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.quantization.gptq] avg loss: 4.781982898712158\n",
            "INFO - Quantizing self_attn.q_proj in layer 25/28...\n",
            "2025-03-26 05:55:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 25/28...\n",
            "2025-03-26 05:55:47 INFO [auto_gptq.quantization.gptq] duration: 0.5924954414367676\n",
            "2025-03-26 05:55:47 INFO [auto_gptq.quantization.gptq] avg loss: 8.202856063842773\n",
            "INFO - Quantizing self_attn.o_proj in layer 25/28...\n",
            "2025-03-26 05:55:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 25/28...\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.quantization.gptq] duration: 0.7371091842651367\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.quantization.gptq] avg loss: 1.2945060729980469\n",
            "INFO - Quantizing mlp.up_proj in layer 25/28...\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 25/28...\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.quantization.gptq] duration: 0.8646199703216553\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.quantization.gptq] avg loss: 31.506437301635742\n",
            "INFO - Quantizing mlp.gate_proj in layer 25/28...\n",
            "2025-03-26 05:55:48 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 25/28...\n",
            "2025-03-26 05:55:49 INFO [auto_gptq.quantization.gptq] duration: 0.6170196533203125\n",
            "2025-03-26 05:55:49 INFO [auto_gptq.quantization.gptq] avg loss: 29.449430465698242\n",
            "INFO - Quantizing mlp.down_proj in layer 25/28...\n",
            "2025-03-26 05:55:49 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 25/28...\n",
            "2025-03-26 05:55:55 INFO [auto_gptq.quantization.gptq] duration: 5.765838384628296\n",
            "2025-03-26 05:55:55 INFO [auto_gptq.quantization.gptq] avg loss: 6.011457920074463\n",
            "INFO - Start quantizing layer 26/28\n",
            "2025-03-26 05:55:55 INFO [auto_gptq.modeling._base] Start quantizing layer 26/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 26/28...\n",
            "2025-03-26 05:55:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 26/28...\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.quantization.gptq] duration: 0.816056489944458\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.quantization.gptq] avg loss: 1.0587074756622314\n",
            "INFO - Quantizing self_attn.v_proj in layer 26/28...\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 26/28...\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.quantization.gptq] duration: 0.5812516212463379\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.quantization.gptq] avg loss: 4.807868480682373\n",
            "INFO - Quantizing self_attn.q_proj in layer 26/28...\n",
            "2025-03-26 05:55:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 26/28...\n",
            "2025-03-26 05:55:57 INFO [auto_gptq.quantization.gptq] duration: 0.6043024063110352\n",
            "2025-03-26 05:55:57 INFO [auto_gptq.quantization.gptq] avg loss: 9.976273536682129\n",
            "INFO - Quantizing self_attn.o_proj in layer 26/28...\n",
            "2025-03-26 05:55:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 26/28...\n",
            "2025-03-26 05:55:58 INFO [auto_gptq.quantization.gptq] duration: 0.7745304107666016\n",
            "2025-03-26 05:55:58 INFO [auto_gptq.quantization.gptq] avg loss: 2.0375959873199463\n",
            "INFO - Quantizing mlp.up_proj in layer 26/28...\n",
            "2025-03-26 05:55:58 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 26/28...\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.quantization.gptq] duration: 0.7784068584442139\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.quantization.gptq] avg loss: 33.0199089050293\n",
            "INFO - Quantizing mlp.gate_proj in layer 26/28...\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 26/28...\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.quantization.gptq] duration: 0.6090133190155029\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.quantization.gptq] avg loss: 28.647052764892578\n",
            "INFO - Quantizing mlp.down_proj in layer 26/28...\n",
            "2025-03-26 05:55:59 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 26/28...\n",
            "2025-03-26 05:56:05 INFO [auto_gptq.quantization.gptq] duration: 5.2196080684661865\n",
            "2025-03-26 05:56:05 INFO [auto_gptq.quantization.gptq] avg loss: 7.1486921310424805\n",
            "INFO - Start quantizing layer 27/28\n",
            "2025-03-26 05:56:05 INFO [auto_gptq.modeling._base] Start quantizing layer 27/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 27/28...\n",
            "2025-03-26 05:56:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 27/28...\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.quantization.gptq] duration: 0.9165661334991455\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.quantization.gptq] avg loss: 1.146368145942688\n",
            "INFO - Quantizing self_attn.v_proj in layer 27/28...\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 27/28...\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.quantization.gptq] duration: 0.7543940544128418\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.quantization.gptq] avg loss: 6.756742000579834\n",
            "INFO - Quantizing self_attn.q_proj in layer 27/28...\n",
            "2025-03-26 05:56:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 27/28...\n",
            "2025-03-26 05:56:07 INFO [auto_gptq.quantization.gptq] duration: 0.8984084129333496\n",
            "2025-03-26 05:56:07 INFO [auto_gptq.quantization.gptq] avg loss: 9.008119583129883\n",
            "INFO - Quantizing self_attn.o_proj in layer 27/28...\n",
            "2025-03-26 05:56:07 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 27/28...\n",
            "2025-03-26 05:56:08 INFO [auto_gptq.quantization.gptq] duration: 0.773242712020874\n",
            "2025-03-26 05:56:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.3968143463134766\n",
            "INFO - Quantizing mlp.up_proj in layer 27/28...\n",
            "2025-03-26 05:56:08 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 27/28...\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.quantization.gptq] duration: 0.7658183574676514\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.quantization.gptq] avg loss: 35.27324676513672\n",
            "INFO - Quantizing mlp.gate_proj in layer 27/28...\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 27/28...\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.quantization.gptq] duration: 0.6277742385864258\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.quantization.gptq] avg loss: 29.3718318939209\n",
            "INFO - Quantizing mlp.down_proj in layer 27/28...\n",
            "2025-03-26 05:56:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 27/28...\n",
            "2025-03-26 05:56:15 INFO [auto_gptq.quantization.gptq] duration: 5.20724081993103\n",
            "2025-03-26 05:56:15 INFO [auto_gptq.quantization.gptq] avg loss: 1382.4183349609375\n",
            "INFO - Start quantizing layer 28/28\n",
            "2025-03-26 05:56:15 INFO [auto_gptq.modeling._base] Start quantizing layer 28/28\n",
            "INFO - Quantizing self_attn.k_proj in layer 28/28...\n",
            "2025-03-26 05:56:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 28/28...\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.quantization.gptq] duration: 0.8139584064483643\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.9145635366439819\n",
            "INFO - Quantizing self_attn.v_proj in layer 28/28...\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 28/28...\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.quantization.gptq] duration: 0.5916926860809326\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.quantization.gptq] avg loss: 6.718857288360596\n",
            "INFO - Quantizing self_attn.q_proj in layer 28/28...\n",
            "2025-03-26 05:56:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 28/28...\n",
            "2025-03-26 05:56:17 INFO [auto_gptq.quantization.gptq] duration: 0.5915806293487549\n",
            "2025-03-26 05:56:17 INFO [auto_gptq.quantization.gptq] avg loss: 8.555377006530762\n",
            "INFO - Quantizing self_attn.o_proj in layer 28/28...\n",
            "2025-03-26 05:56:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 28/28...\n",
            "2025-03-26 05:56:18 INFO [auto_gptq.quantization.gptq] duration: 0.9213171005249023\n",
            "2025-03-26 05:56:18 INFO [auto_gptq.quantization.gptq] avg loss: 4.875669956207275\n",
            "INFO - Quantizing mlp.up_proj in layer 28/28...\n",
            "2025-03-26 05:56:18 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 28/28...\n",
            "2025-03-26 05:56:19 INFO [auto_gptq.quantization.gptq] duration: 0.9819319248199463\n",
            "2025-03-26 05:56:19 INFO [auto_gptq.quantization.gptq] avg loss: 43.286102294921875\n",
            "INFO - Quantizing mlp.gate_proj in layer 28/28...\n",
            "2025-03-26 05:56:19 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 28/28...\n",
            "2025-03-26 05:56:20 INFO [auto_gptq.quantization.gptq] duration: 0.8049240112304688\n",
            "2025-03-26 05:56:20 INFO [auto_gptq.quantization.gptq] avg loss: 41.98606872558594\n",
            "INFO - Quantizing mlp.down_proj in layer 28/28...\n",
            "2025-03-26 05:56:20 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 28/28...\n",
            "2025-03-26 05:56:25 INFO [auto_gptq.quantization.gptq] duration: 5.263568162918091\n",
            "2025-03-26 05:56:25 INFO [auto_gptq.quantization.gptq] avg loss: 25.359046936035156\n",
            "2025-03-26 05:56:25 INFO [auto_gptq.modeling._utils] Packing model...\n",
            "2025-03-26 05:56:26 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
            "2025-03-26 05:56:26 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
            "2025-03-26 05:56:26 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
            "2025-03-26 05:56:26 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
            "2025-03-26 05:56:26 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
            "2025-03-26 05:56:27 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
            "2025-03-26 05:56:27 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
            "2025-03-26 05:56:28 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
            "2025-03-26 05:56:28 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
            "2025-03-26 05:56:28 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
            "2025-03-26 05:56:28 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
            "2025-03-26 05:56:28 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
            "2025-03-26 05:56:29 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
            "2025-03-26 05:56:29 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
            "2025-03-26 05:56:30 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
            "2025-03-26 05:56:31 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
            "2025-03-26 05:56:31 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
            "2025-03-26 05:56:31 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
            "2025-03-26 05:56:31 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
            "2025-03-26 05:56:32 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
            "2025-03-26 05:56:32 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
            "2025-03-26 05:56:33 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
            "2025-03-26 05:56:33 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
            "2025-03-26 05:56:33 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
            "2025-03-26 05:56:33 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
            "2025-03-26 05:56:33 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
            "2025-03-26 05:56:34 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
            "2025-03-26 05:56:34 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
            "2025-03-26 05:56:35 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
            "2025-03-26 05:56:35 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
            "2025-03-26 05:56:35 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
            "2025-03-26 05:56:35 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
            "2025-03-26 05:56:36 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
            "2025-03-26 05:56:36 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
            "2025-03-26 05:56:37 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
            "2025-03-26 05:56:37 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
            "2025-03-26 05:56:37 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
            "2025-03-26 05:56:38 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
            "2025-03-26 05:56:38 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
            "2025-03-26 05:56:38 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
            "2025-03-26 05:56:38 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
            "2025-03-26 05:56:39 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
            "2025-03-26 05:56:40 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
            "2025-03-26 05:56:41 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
            "2025-03-26 05:56:42 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
            "2025-03-26 05:56:42 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
            "2025-03-26 05:56:42 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
            "2025-03-26 05:56:42 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
            "2025-03-26 05:56:42 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
            "2025-03-26 05:56:43 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
            "2025-03-26 05:56:43 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
            "2025-03-26 05:56:44 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
            "2025-03-26 05:56:44 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
            "2025-03-26 05:56:45 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
            "2025-03-26 05:56:45 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
            "2025-03-26 05:56:45 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
            "2025-03-26 05:56:45 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
            "2025-03-26 05:56:46 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
            "2025-03-26 05:56:46 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
            "2025-03-26 05:56:47 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
            "2025-03-26 05:56:47 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
            "2025-03-26 05:56:47 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
            "2025-03-26 05:56:47 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
            "2025-03-26 05:56:47 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
            "2025-03-26 05:56:48 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
            "2025-03-26 05:56:48 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
            "2025-03-26 05:56:49 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
            "2025-03-26 05:56:49 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
            "2025-03-26 05:56:49 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
            "2025-03-26 05:56:49 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
            "2025-03-26 05:56:49 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
            "2025-03-26 05:56:50 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
            "2025-03-26 05:56:51 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
            "2025-03-26 05:56:52 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
            "2025-03-26 05:56:53 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
            "2025-03-26 05:56:54 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
            "2025-03-26 05:56:55 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
            "2025-03-26 05:56:55 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
            "2025-03-26 05:56:55 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
            "2025-03-26 05:56:55 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
            "2025-03-26 05:56:55 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
            "2025-03-26 05:56:56 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
            "2025-03-26 05:56:57 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
            "2025-03-26 05:56:58 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
            "2025-03-26 05:56:58 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
            "2025-03-26 05:56:58 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
            "2025-03-26 05:56:58 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
            "2025-03-26 05:56:58 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
            "2025-03-26 05:56:59 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
            "2025-03-26 05:56:59 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
            "2025-03-26 05:57:00 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
            "2025-03-26 05:57:00 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
            "2025-03-26 05:57:00 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
            "2025-03-26 05:57:00 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
            "2025-03-26 05:57:01 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
            "2025-03-26 05:57:01 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
            "2025-03-26 05:57:02 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
            "2025-03-26 05:57:02 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.q_proj\n",
            "2025-03-26 05:57:03 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.k_proj\n",
            "2025-03-26 05:57:03 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.v_proj\n",
            "2025-03-26 05:57:03 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.o_proj\n",
            "2025-03-26 05:57:03 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.gate_proj\n",
            "2025-03-26 05:57:03 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.up_proj\n",
            "2025-03-26 05:57:04 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.down_proj\n",
            "2025-03-26 05:57:05 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.q_proj\n",
            "2025-03-26 05:57:05 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.k_proj\n",
            "2025-03-26 05:57:05 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.v_proj\n",
            "2025-03-26 05:57:05 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.o_proj\n",
            "2025-03-26 05:57:05 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.gate_proj\n",
            "2025-03-26 05:57:06 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.up_proj\n",
            "2025-03-26 05:57:06 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.down_proj\n",
            "2025-03-26 05:57:07 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.q_proj\n",
            "2025-03-26 05:57:07 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.k_proj\n",
            "2025-03-26 05:57:07 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.v_proj\n",
            "2025-03-26 05:57:07 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.o_proj\n",
            "2025-03-26 05:57:07 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.gate_proj\n",
            "2025-03-26 05:57:08 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.up_proj\n",
            "2025-03-26 05:57:09 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.down_proj\n",
            "2025-03-26 05:57:10 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.q_proj\n",
            "2025-03-26 05:57:10 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.k_proj\n",
            "2025-03-26 05:57:10 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.v_proj\n",
            "2025-03-26 05:57:10 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.o_proj\n",
            "2025-03-26 05:57:10 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.gate_proj\n",
            "2025-03-26 05:57:11 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.up_proj\n",
            "2025-03-26 05:57:11 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.down_proj\n",
            "2025-03-26 05:57:12 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.q_proj\n",
            "2025-03-26 05:57:12 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.k_proj\n",
            "2025-03-26 05:57:12 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.v_proj\n",
            "2025-03-26 05:57:12 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.o_proj\n",
            "2025-03-26 05:57:12 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.gate_proj\n",
            "2025-03-26 05:57:13 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.up_proj\n",
            "2025-03-26 05:57:13 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.down_proj\n",
            "2025-03-26 05:57:14 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.q_proj\n",
            "2025-03-26 05:57:14 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.k_proj\n",
            "2025-03-26 05:57:14 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.v_proj\n",
            "2025-03-26 05:57:14 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.o_proj\n",
            "2025-03-26 05:57:14 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.gate_proj\n",
            "2025-03-26 05:57:15 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.up_proj\n",
            "2025-03-26 05:57:16 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.down_proj\n",
            "2025-03-26 05:57:16 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.q_proj\n",
            "2025-03-26 05:57:16 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.k_proj\n",
            "2025-03-26 05:57:16 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.v_proj\n",
            "2025-03-26 05:57:17 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.o_proj\n",
            "2025-03-26 05:57:17 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.gate_proj\n",
            "2025-03-26 05:57:17 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.up_proj\n",
            "2025-03-26 05:57:18 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.down_proj\n",
            "2025-03-26 05:57:19 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.q_proj\n",
            "2025-03-26 05:57:19 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.k_proj\n",
            "2025-03-26 05:57:19 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.v_proj\n",
            "2025-03-26 05:57:19 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.o_proj\n",
            "2025-03-26 05:57:19 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.gate_proj\n",
            "2025-03-26 05:57:20 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.up_proj\n",
            "2025-03-26 05:57:20 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.down_proj\n",
            "2025-03-26 05:57:22 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.q_proj\n",
            "2025-03-26 05:57:22 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.k_proj\n",
            "2025-03-26 05:57:22 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.v_proj\n",
            "2025-03-26 05:57:22 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.o_proj\n",
            "2025-03-26 05:57:22 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.gate_proj\n",
            "2025-03-26 05:57:23 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.up_proj\n",
            "2025-03-26 05:57:23 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.down_proj\n",
            "2025-03-26 05:57:24 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.q_proj\n",
            "2025-03-26 05:57:24 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.k_proj\n",
            "2025-03-26 05:57:24 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.v_proj\n",
            "2025-03-26 05:57:24 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.o_proj\n",
            "2025-03-26 05:57:25 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.gate_proj\n",
            "2025-03-26 05:57:25 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.up_proj\n",
            "2025-03-26 05:57:26 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.down_proj\n",
            "2025-03-26 05:57:26 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.q_proj\n",
            "2025-03-26 05:57:27 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.k_proj\n",
            "2025-03-26 05:57:27 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.v_proj\n",
            "2025-03-26 05:57:27 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.o_proj\n",
            "2025-03-26 05:57:27 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.gate_proj\n",
            "2025-03-26 05:57:27 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.up_proj\n",
            "2025-03-26 05:57:28 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.down_proj\n",
            "2025-03-26 05:57:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.q_proj\n",
            "2025-03-26 05:57:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.k_proj\n",
            "2025-03-26 05:57:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.v_proj\n",
            "2025-03-26 05:57:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.o_proj\n",
            "2025-03-26 05:57:29 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.gate_proj\n",
            "2025-03-26 05:57:30 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.up_proj\n",
            "2025-03-26 05:57:30 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.down_proj\n",
            "2025-03-26 05:57:31 INFO [auto_gptq.modeling._utils] Model packed.\n",
            "quantization took:  350.0461s\n",
            "Device set to use cuda:0\n",
            "The model 'Qwen2GPTQForCausalLM' is not supported for . Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
            "prompt: Instruction:\n",
            "Localize the following text for German\n",
            "Input:\n",
            "The weather is great today\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Das Wetter ist heute toll.\n",
            "------------------------------------------\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=155) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "quant: Die Klima ist heute groß\n",
            "You are an AI assistant. Provide a detailed answer so user don’t need to ask follow up question.\n",
            "generate 29 tokens using  5.4940s, 5.27846641951189 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Write the code for a function that takes an array of integers and returns the sum.\n",
            "Input:\n",
            "[1,2,3,4,5]\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: def sum_array(arr): \n",
            "  total = 0\n",
            "  for x in arr: \n",
            "    total += x \n",
            "  return total\n",
            "------------------------------------------\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=189) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "quant: 15\n",
            "To solve this problem, we can use a simple for loop to iterate through the array and add each element to a running total. Here's the code:\n",
            "\n",
            "```python\n",
            "def sum_array(arr):\n",
            "    total = 0\n",
            "    for num in arr:\n",
            "        total += num\n",
            "    return total\n",
            "\n",
            "# Test the function\n",
            "arr = [1, 2, 3, 4, 5]\n",
            "print(sum_array(arr))  # Output: 15\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "- We initialize a variable `total` to 0, which will hold the running total of the array elements.\n",
            "- We then iterate through each element `num` in the array `arr`.\n",
            "- Inside the loop, we add `num` to the `total` variable.\n",
            "- After the loop finishes, we return the `total` variable, which will contain the sum of all the array elements.\n",
            "generate 183 tokens using  31.7799s, 5.758349166916839 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Create a query to retrieve all items in a database with a status of 'shipped'.\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: SELECT * FROM orders WHERE status = 'shipped';\n",
            "------------------------------------------\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=161) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "quant: SELECT * FROM items WHERE status ='shipped';\n",
            "generate 10 tokens using  2.0536s, 4.869407407974986 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Given a satirical news article, write a 1 sentence summary of the article.\n",
            "Input:\n",
            "Article: A new study finds that eating pop tarts can help children score higher grades in school\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: A new study suggests that an unhealthy diet of pop tarts could be the key to academic excellence.\n",
            "------------------------------------------\n",
            "Both `max_new_tokens` (=2048) and `max_length`(=191) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "quant: Eating pop tarts can improve children's academic performance.\n",
            "generate 12 tokens using  2.2090s, 5.4324199618545 tokens/s.\n",
            "==========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "بالتأكيد، سأشرح لك معنى --per_gpu_max_memory 4 في سياق الأمر python quant_with_alpaca.py --pretrained_model_dir \"facebook/opt-125m\" --per_gpu_max_memory 4 --quant_batch_size 16\n",
        "\n",
        "ما هو --per_gpu_max_memory؟\n",
        "\n",
        "--per_gpu_max_memory هو خيار (argument) يستخدم في برنامج quant_with_alpaca.py لتحديد مقدار الذاكرة (RAM) التي يمكن لكل وحدة معالجة رسومية (GPU) استخدامها أثناء عملية التكميم (quantization).\n",
        "\n",
        "--per_gpu_max_memory 4: يعني أنك تحدد أن كل وحدة GPU يجب ألا تستخدم أكثر من 4 جيجابايت (GB) من الذاكرة.\n",
        "\n",
        "لماذا نحدد هذا الخيا"
      ],
      "metadata": {
        "id": "3bdVExSUBCuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/quantization\n",
        "!python quant_with_alpaca.py --pretrained_model_dir \"meta-llama/Llama-3.2-1B\" --per_gpu_max_memory 6 --quant_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf7FRI_-BD5n",
        "outputId": "630e7a3b-0d5e-42bd-800f-b1139a16b886"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/quantization\n",
            "2025-03-26 05:58:48.111396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742968728.146853   19434 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742968728.158401   19434 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 05:58:48.193598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Generating train split: 128 examples [00:00, 43296.04 examples/s]\n",
            "Map: 100% 128/128 [00:00<00:00, 2127.26 examples/s]\n",
            "INFO - Start quantizing layer 1/16\n",
            "2025-03-26 05:59:17 INFO [auto_gptq.modeling._base] Start quantizing layer 1/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 1/16...\n",
            "2025-03-26 05:59:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/16...\n",
            "2025-03-26 05:59:19 INFO [auto_gptq.quantization.gptq] duration: 1.5196483135223389\n",
            "2025-03-26 05:59:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.38227736949920654\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/16...\n",
            "2025-03-26 05:59:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/16...\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.quantization.gptq] duration: 0.7810177803039551\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.011474201455712318\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/16...\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/16...\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.quantization.gptq] duration: 0.8235814571380615\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.8089473843574524\n",
            "INFO - Quantizing self_attn.o_proj in layer 1/16...\n",
            "2025-03-26 05:59:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/16...\n",
            "2025-03-26 05:59:22 INFO [auto_gptq.quantization.gptq] duration: 1.3037786483764648\n",
            "2025-03-26 05:59:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.001413461985066533\n",
            "INFO - Quantizing mlp.up_proj in layer 1/16...\n",
            "2025-03-26 05:59:22 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/16...\n",
            "2025-03-26 05:59:23 INFO [auto_gptq.quantization.gptq] duration: 1.3635311126708984\n",
            "2025-03-26 05:59:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.5812485814094543\n",
            "INFO - Quantizing mlp.gate_proj in layer 1/16...\n",
            "2025-03-26 05:59:23 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/16...\n",
            "2025-03-26 05:59:24 INFO [auto_gptq.quantization.gptq] duration: 0.7969155311584473\n",
            "2025-03-26 05:59:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.7498282194137573\n",
            "INFO - Quantizing mlp.down_proj in layer 1/16...\n",
            "2025-03-26 05:59:24 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/16...\n",
            "2025-03-26 05:59:29 INFO [auto_gptq.quantization.gptq] duration: 4.605723857879639\n",
            "2025-03-26 05:59:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.006582954898476601\n",
            "INFO - Start quantizing layer 2/16\n",
            "2025-03-26 05:59:29 INFO [auto_gptq.modeling._base] Start quantizing layer 2/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/16...\n",
            "2025-03-26 05:59:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/16...\n",
            "2025-03-26 05:59:30 INFO [auto_gptq.quantization.gptq] duration: 1.1492974758148193\n",
            "2025-03-26 05:59:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.7092311382293701\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/16...\n",
            "2025-03-26 05:59:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/16...\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.quantization.gptq] duration: 0.7785277366638184\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.04248017445206642\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/16...\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/16...\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.quantization.gptq] duration: 0.769050121307373\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.quantization.gptq] avg loss: 1.449810266494751\n",
            "INFO - Quantizing self_attn.o_proj in layer 2/16...\n",
            "2025-03-26 05:59:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/16...\n",
            "2025-03-26 05:59:32 INFO [auto_gptq.quantization.gptq] duration: 1.0171549320220947\n",
            "2025-03-26 05:59:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.00521449651569128\n",
            "INFO - Quantizing mlp.up_proj in layer 2/16...\n",
            "2025-03-26 05:59:32 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/16...\n",
            "2025-03-26 05:59:34 INFO [auto_gptq.quantization.gptq] duration: 1.307361125946045\n",
            "2025-03-26 05:59:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.8925336599349976\n",
            "INFO - Quantizing mlp.gate_proj in layer 2/16...\n",
            "2025-03-26 05:59:34 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/16...\n",
            "2025-03-26 05:59:35 INFO [auto_gptq.quantization.gptq] duration: 1.1345441341400146\n",
            "2025-03-26 05:59:35 INFO [auto_gptq.quantization.gptq] avg loss: 1.218217134475708\n",
            "INFO - Quantizing mlp.down_proj in layer 2/16...\n",
            "2025-03-26 05:59:35 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/16...\n",
            "2025-03-26 05:59:40 INFO [auto_gptq.quantization.gptq] duration: 4.628187656402588\n",
            "2025-03-26 05:59:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.37292325496673584\n",
            "INFO - Start quantizing layer 3/16\n",
            "2025-03-26 05:59:40 INFO [auto_gptq.modeling._base] Start quantizing layer 3/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/16...\n",
            "2025-03-26 05:59:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/16...\n",
            "2025-03-26 05:59:41 INFO [auto_gptq.quantization.gptq] duration: 1.107323169708252\n",
            "2025-03-26 05:59:41 INFO [auto_gptq.quantization.gptq] avg loss: 1.4125628471374512\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/16...\n",
            "2025-03-26 05:59:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/16...\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.quantization.gptq] duration: 0.7757890224456787\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.10798347741365433\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/16...\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/16...\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.quantization.gptq] duration: 0.7987213134765625\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.quantization.gptq] avg loss: 2.8047919273376465\n",
            "INFO - Quantizing self_attn.o_proj in layer 3/16...\n",
            "2025-03-26 05:59:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/16...\n",
            "2025-03-26 05:59:43 INFO [auto_gptq.quantization.gptq] duration: 1.0297749042510986\n",
            "2025-03-26 05:59:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.006088307127356529\n",
            "INFO - Quantizing mlp.up_proj in layer 3/16...\n",
            "2025-03-26 05:59:44 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/16...\n",
            "2025-03-26 05:59:45 INFO [auto_gptq.quantization.gptq] duration: 1.0640740394592285\n",
            "2025-03-26 05:59:45 INFO [auto_gptq.quantization.gptq] avg loss: 1.1194682121276855\n",
            "INFO - Quantizing mlp.gate_proj in layer 3/16...\n",
            "2025-03-26 05:59:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/16...\n",
            "2025-03-26 05:59:46 INFO [auto_gptq.quantization.gptq] duration: 0.9843511581420898\n",
            "2025-03-26 05:59:46 INFO [auto_gptq.quantization.gptq] avg loss: 1.7897285223007202\n",
            "INFO - Quantizing mlp.down_proj in layer 3/16...\n",
            "2025-03-26 05:59:46 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/16...\n",
            "2025-03-26 05:59:50 INFO [auto_gptq.quantization.gptq] duration: 4.597290992736816\n",
            "2025-03-26 05:59:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.011418603360652924\n",
            "INFO - Start quantizing layer 4/16\n",
            "2025-03-26 05:59:50 INFO [auto_gptq.modeling._base] Start quantizing layer 4/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/16...\n",
            "2025-03-26 05:59:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/16...\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.quantization.gptq] duration: 1.1304535865783691\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.7945398092269897\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/16...\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/16...\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.quantization.gptq] duration: 0.7878320217132568\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.1185055524110794\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/16...\n",
            "2025-03-26 05:59:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/16...\n",
            "2025-03-26 05:59:53 INFO [auto_gptq.quantization.gptq] duration: 0.7884056568145752\n",
            "2025-03-26 05:59:53 INFO [auto_gptq.quantization.gptq] avg loss: 1.7710330486297607\n",
            "INFO - Quantizing self_attn.o_proj in layer 4/16...\n",
            "2025-03-26 05:59:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/16...\n",
            "2025-03-26 05:59:54 INFO [auto_gptq.quantization.gptq] duration: 0.9958901405334473\n",
            "2025-03-26 05:59:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.008541911840438843\n",
            "INFO - Quantizing mlp.up_proj in layer 4/16...\n",
            "2025-03-26 05:59:54 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/16...\n",
            "2025-03-26 05:59:55 INFO [auto_gptq.quantization.gptq] duration: 1.043724536895752\n",
            "2025-03-26 05:59:55 INFO [auto_gptq.quantization.gptq] avg loss: 1.3460140228271484\n",
            "INFO - Quantizing mlp.gate_proj in layer 4/16...\n",
            "2025-03-26 05:59:55 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/16...\n",
            "2025-03-26 05:59:56 INFO [auto_gptq.quantization.gptq] duration: 0.7893333435058594\n",
            "2025-03-26 05:59:56 INFO [auto_gptq.quantization.gptq] avg loss: 2.6776509284973145\n",
            "INFO - Quantizing mlp.down_proj in layer 4/16...\n",
            "2025-03-26 05:59:56 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/16...\n",
            "2025-03-26 06:00:01 INFO [auto_gptq.quantization.gptq] duration: 5.0776636600494385\n",
            "2025-03-26 06:00:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.016294844448566437\n",
            "INFO - Start quantizing layer 5/16\n",
            "2025-03-26 06:00:01 INFO [auto_gptq.modeling._base] Start quantizing layer 5/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/16...\n",
            "2025-03-26 06:00:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/16...\n",
            "2025-03-26 06:00:02 INFO [auto_gptq.quantization.gptq] duration: 1.1171669960021973\n",
            "2025-03-26 06:00:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.8891681432723999\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/16...\n",
            "2025-03-26 06:00:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/16...\n",
            "2025-03-26 06:00:03 INFO [auto_gptq.quantization.gptq] duration: 0.797600507736206\n",
            "2025-03-26 06:00:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.11224816739559174\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/16...\n",
            "2025-03-26 06:00:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/16...\n",
            "2025-03-26 06:00:04 INFO [auto_gptq.quantization.gptq] duration: 0.7904930114746094\n",
            "2025-03-26 06:00:04 INFO [auto_gptq.quantization.gptq] avg loss: 1.8087764978408813\n",
            "INFO - Quantizing self_attn.o_proj in layer 5/16...\n",
            "2025-03-26 06:00:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/16...\n",
            "2025-03-26 06:00:05 INFO [auto_gptq.quantization.gptq] duration: 1.029719352722168\n",
            "2025-03-26 06:00:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.01043969951570034\n",
            "INFO - Quantizing mlp.up_proj in layer 5/16...\n",
            "2025-03-26 06:00:05 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/16...\n",
            "2025-03-26 06:00:06 INFO [auto_gptq.quantization.gptq] duration: 1.0821983814239502\n",
            "2025-03-26 06:00:06 INFO [auto_gptq.quantization.gptq] avg loss: 1.3556424379348755\n",
            "INFO - Quantizing mlp.gate_proj in layer 5/16...\n",
            "2025-03-26 06:00:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/16...\n",
            "2025-03-26 06:00:07 INFO [auto_gptq.quantization.gptq] duration: 0.799370527267456\n",
            "2025-03-26 06:00:07 INFO [auto_gptq.quantization.gptq] avg loss: 2.9013562202453613\n",
            "INFO - Quantizing mlp.down_proj in layer 5/16...\n",
            "2025-03-26 06:00:07 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/16...\n",
            "2025-03-26 06:00:12 INFO [auto_gptq.quantization.gptq] duration: 5.256818771362305\n",
            "2025-03-26 06:00:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.01766183227300644\n",
            "INFO - Start quantizing layer 6/16\n",
            "2025-03-26 06:00:12 INFO [auto_gptq.modeling._base] Start quantizing layer 6/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/16...\n",
            "2025-03-26 06:00:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/16...\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.quantization.gptq] duration: 1.1614775657653809\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.quantization.gptq] avg loss: 1.30410635471344\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/16...\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/16...\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.quantization.gptq] duration: 0.7833652496337891\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.10649436712265015\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/16...\n",
            "2025-03-26 06:00:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/16...\n",
            "2025-03-26 06:00:15 INFO [auto_gptq.quantization.gptq] duration: 0.7849736213684082\n",
            "2025-03-26 06:00:15 INFO [auto_gptq.quantization.gptq] avg loss: 2.4499096870422363\n",
            "INFO - Quantizing self_attn.o_proj in layer 6/16...\n",
            "2025-03-26 06:00:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/16...\n",
            "2025-03-26 06:00:16 INFO [auto_gptq.quantization.gptq] duration: 1.0228700637817383\n",
            "2025-03-26 06:00:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.011393213644623756\n",
            "INFO - Quantizing mlp.up_proj in layer 6/16...\n",
            "2025-03-26 06:00:16 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/16...\n",
            "2025-03-26 06:00:17 INFO [auto_gptq.quantization.gptq] duration: 1.0854201316833496\n",
            "2025-03-26 06:00:17 INFO [auto_gptq.quantization.gptq] avg loss: 1.4087011814117432\n",
            "INFO - Quantizing mlp.gate_proj in layer 6/16...\n",
            "2025-03-26 06:00:17 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/16...\n",
            "2025-03-26 06:00:18 INFO [auto_gptq.quantization.gptq] duration: 0.8009858131408691\n",
            "2025-03-26 06:00:18 INFO [auto_gptq.quantization.gptq] avg loss: 2.586308717727661\n",
            "INFO - Quantizing mlp.down_proj in layer 6/16...\n",
            "2025-03-26 06:00:18 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/16...\n",
            "2025-03-26 06:00:23 INFO [auto_gptq.quantization.gptq] duration: 5.323931694030762\n",
            "2025-03-26 06:00:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.020425526425242424\n",
            "INFO - Start quantizing layer 7/16\n",
            "2025-03-26 06:00:23 INFO [auto_gptq.modeling._base] Start quantizing layer 7/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/16...\n",
            "2025-03-26 06:00:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/16...\n",
            "2025-03-26 06:00:25 INFO [auto_gptq.quantization.gptq] duration: 1.3348462581634521\n",
            "2025-03-26 06:00:25 INFO [auto_gptq.quantization.gptq] avg loss: 1.0041487216949463\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/16...\n",
            "2025-03-26 06:00:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/16...\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.quantization.gptq] duration: 0.7910053730010986\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.12223179638385773\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/16...\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/16...\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.quantization.gptq] duration: 0.7900543212890625\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.quantization.gptq] avg loss: 1.6191411018371582\n",
            "INFO - Quantizing self_attn.o_proj in layer 7/16...\n",
            "2025-03-26 06:00:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/16...\n",
            "2025-03-26 06:00:27 INFO [auto_gptq.quantization.gptq] duration: 1.023759126663208\n",
            "2025-03-26 06:00:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.019894735887646675\n",
            "INFO - Quantizing mlp.up_proj in layer 7/16...\n",
            "2025-03-26 06:00:27 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/16...\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.quantization.gptq] duration: 1.0941879749298096\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.quantization.gptq] avg loss: 1.385565996170044\n",
            "INFO - Quantizing mlp.gate_proj in layer 7/16...\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/16...\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.quantization.gptq] duration: 0.8015391826629639\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.quantization.gptq] avg loss: 2.5098392963409424\n",
            "INFO - Quantizing mlp.down_proj in layer 7/16...\n",
            "2025-03-26 06:00:29 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/16...\n",
            "2025-03-26 06:00:34 INFO [auto_gptq.quantization.gptq] duration: 5.112105846405029\n",
            "2025-03-26 06:00:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.019926806911826134\n",
            "INFO - Start quantizing layer 8/16\n",
            "2025-03-26 06:00:35 INFO [auto_gptq.modeling._base] Start quantizing layer 8/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/16...\n",
            "2025-03-26 06:00:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/16...\n",
            "2025-03-26 06:00:36 INFO [auto_gptq.quantization.gptq] duration: 1.1423709392547607\n",
            "2025-03-26 06:00:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.9523978233337402\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/16...\n",
            "2025-03-26 06:00:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/16...\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.quantization.gptq] duration: 0.7952835559844971\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.12738831341266632\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/16...\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/16...\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.quantization.gptq] duration: 0.7780513763427734\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.quantization.gptq] avg loss: 1.7816296815872192\n",
            "INFO - Quantizing self_attn.o_proj in layer 8/16...\n",
            "2025-03-26 06:00:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/16...\n",
            "2025-03-26 06:00:38 INFO [auto_gptq.quantization.gptq] duration: 1.011991262435913\n",
            "2025-03-26 06:00:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.013996504247188568\n",
            "INFO - Quantizing mlp.up_proj in layer 8/16...\n",
            "2025-03-26 06:00:38 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/16...\n",
            "2025-03-26 06:00:39 INFO [auto_gptq.quantization.gptq] duration: 1.067319631576538\n",
            "2025-03-26 06:00:39 INFO [auto_gptq.quantization.gptq] avg loss: 1.458085060119629\n",
            "INFO - Quantizing mlp.gate_proj in layer 8/16...\n",
            "2025-03-26 06:00:39 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/16...\n",
            "2025-03-26 06:00:40 INFO [auto_gptq.quantization.gptq] duration: 0.8002746105194092\n",
            "2025-03-26 06:00:40 INFO [auto_gptq.quantization.gptq] avg loss: 2.4069948196411133\n",
            "INFO - Quantizing mlp.down_proj in layer 8/16...\n",
            "2025-03-26 06:00:40 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/16...\n",
            "2025-03-26 06:00:45 INFO [auto_gptq.quantization.gptq] duration: 4.6551220417022705\n",
            "2025-03-26 06:00:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.023770449683070183\n",
            "INFO - Start quantizing layer 9/16\n",
            "2025-03-26 06:00:45 INFO [auto_gptq.modeling._base] Start quantizing layer 9/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/16...\n",
            "2025-03-26 06:00:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/16...\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.quantization.gptq] duration: 1.5630788803100586\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.quantization.gptq] avg loss: 1.2670776844024658\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/16...\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/16...\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.quantization.gptq] duration: 0.9150338172912598\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.12552857398986816\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/16...\n",
            "2025-03-26 06:00:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/16...\n",
            "2025-03-26 06:00:48 INFO [auto_gptq.quantization.gptq] duration: 0.8043267726898193\n",
            "2025-03-26 06:00:48 INFO [auto_gptq.quantization.gptq] avg loss: 2.0889036655426025\n",
            "INFO - Quantizing self_attn.o_proj in layer 9/16...\n",
            "2025-03-26 06:00:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/16...\n",
            "2025-03-26 06:00:49 INFO [auto_gptq.quantization.gptq] duration: 1.0431571006774902\n",
            "2025-03-26 06:00:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.022770792245864868\n",
            "INFO - Quantizing mlp.up_proj in layer 9/16...\n",
            "2025-03-26 06:00:49 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/16...\n",
            "2025-03-26 06:00:50 INFO [auto_gptq.quantization.gptq] duration: 1.0714061260223389\n",
            "2025-03-26 06:00:50 INFO [auto_gptq.quantization.gptq] avg loss: 1.7149680852890015\n",
            "INFO - Quantizing mlp.gate_proj in layer 9/16...\n",
            "2025-03-26 06:00:50 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/16...\n",
            "2025-03-26 06:00:51 INFO [auto_gptq.quantization.gptq] duration: 0.8013064861297607\n",
            "2025-03-26 06:00:51 INFO [auto_gptq.quantization.gptq] avg loss: 2.8217170238494873\n",
            "INFO - Quantizing mlp.down_proj in layer 9/16...\n",
            "2025-03-26 06:00:51 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/16...\n",
            "2025-03-26 06:00:56 INFO [auto_gptq.quantization.gptq] duration: 5.145472288131714\n",
            "2025-03-26 06:00:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.033946480602025986\n",
            "INFO - Start quantizing layer 10/16\n",
            "2025-03-26 06:00:56 INFO [auto_gptq.modeling._base] Start quantizing layer 10/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/16...\n",
            "2025-03-26 06:00:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/16...\n",
            "2025-03-26 06:00:58 INFO [auto_gptq.quantization.gptq] duration: 1.6170377731323242\n",
            "2025-03-26 06:00:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.9443289041519165\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/16...\n",
            "2025-03-26 06:00:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/16...\n",
            "2025-03-26 06:00:59 INFO [auto_gptq.quantization.gptq] duration: 1.10975980758667\n",
            "2025-03-26 06:00:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.14947208762168884\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/16...\n",
            "2025-03-26 06:00:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/16...\n",
            "2025-03-26 06:01:00 INFO [auto_gptq.quantization.gptq] duration: 0.812891960144043\n",
            "2025-03-26 06:01:00 INFO [auto_gptq.quantization.gptq] avg loss: 2.317081928253174\n",
            "INFO - Quantizing self_attn.o_proj in layer 10/16...\n",
            "2025-03-26 06:01:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/16...\n",
            "2025-03-26 06:01:01 INFO [auto_gptq.quantization.gptq] duration: 1.0322327613830566\n",
            "2025-03-26 06:01:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.02504882588982582\n",
            "INFO - Quantizing mlp.up_proj in layer 10/16...\n",
            "2025-03-26 06:01:01 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/16...\n",
            "2025-03-26 06:01:02 INFO [auto_gptq.quantization.gptq] duration: 1.0581107139587402\n",
            "2025-03-26 06:01:02 INFO [auto_gptq.quantization.gptq] avg loss: 1.8258342742919922\n",
            "INFO - Quantizing mlp.gate_proj in layer 10/16...\n",
            "2025-03-26 06:01:02 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/16...\n",
            "2025-03-26 06:01:03 INFO [auto_gptq.quantization.gptq] duration: 0.8022737503051758\n",
            "2025-03-26 06:01:03 INFO [auto_gptq.quantization.gptq] avg loss: 3.0569074153900146\n",
            "INFO - Quantizing mlp.down_proj in layer 10/16...\n",
            "2025-03-26 06:01:03 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/16...\n",
            "2025-03-26 06:01:08 INFO [auto_gptq.quantization.gptq] duration: 4.645325183868408\n",
            "2025-03-26 06:01:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.04073186218738556\n",
            "INFO - Start quantizing layer 11/16\n",
            "2025-03-26 06:01:08 INFO [auto_gptq.modeling._base] Start quantizing layer 11/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/16...\n",
            "2025-03-26 06:01:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/16...\n",
            "2025-03-26 06:01:09 INFO [auto_gptq.quantization.gptq] duration: 1.305666208267212\n",
            "2025-03-26 06:01:09 INFO [auto_gptq.quantization.gptq] avg loss: 1.1426738500595093\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/16...\n",
            "2025-03-26 06:01:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/16...\n",
            "2025-03-26 06:01:10 INFO [auto_gptq.quantization.gptq] duration: 1.0450761318206787\n",
            "2025-03-26 06:01:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.20423537492752075\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/16...\n",
            "2025-03-26 06:01:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/16...\n",
            "2025-03-26 06:01:11 INFO [auto_gptq.quantization.gptq] duration: 1.0906167030334473\n",
            "2025-03-26 06:01:11 INFO [auto_gptq.quantization.gptq] avg loss: 2.587669849395752\n",
            "INFO - Quantizing self_attn.o_proj in layer 11/16...\n",
            "2025-03-26 06:01:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/16...\n",
            "2025-03-26 06:01:12 INFO [auto_gptq.quantization.gptq] duration: 1.0307667255401611\n",
            "2025-03-26 06:01:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.022351210936903954\n",
            "INFO - Quantizing mlp.up_proj in layer 11/16...\n",
            "2025-03-26 06:01:12 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/16...\n",
            "2025-03-26 06:01:13 INFO [auto_gptq.quantization.gptq] duration: 1.0719420909881592\n",
            "2025-03-26 06:01:13 INFO [auto_gptq.quantization.gptq] avg loss: 2.0921311378479004\n",
            "INFO - Quantizing mlp.gate_proj in layer 11/16...\n",
            "2025-03-26 06:01:13 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/16...\n",
            "2025-03-26 06:01:14 INFO [auto_gptq.quantization.gptq] duration: 0.8019459247589111\n",
            "2025-03-26 06:01:14 INFO [auto_gptq.quantization.gptq] avg loss: 3.3383114337921143\n",
            "INFO - Quantizing mlp.down_proj in layer 11/16...\n",
            "2025-03-26 06:01:14 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/16...\n",
            "2025-03-26 06:01:19 INFO [auto_gptq.quantization.gptq] duration: 4.631409645080566\n",
            "2025-03-26 06:01:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.04978249594569206\n",
            "INFO - Start quantizing layer 12/16\n",
            "2025-03-26 06:01:19 INFO [auto_gptq.modeling._base] Start quantizing layer 12/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/16...\n",
            "2025-03-26 06:01:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/16...\n",
            "2025-03-26 06:01:20 INFO [auto_gptq.quantization.gptq] duration: 1.1313273906707764\n",
            "2025-03-26 06:01:20 INFO [auto_gptq.quantization.gptq] avg loss: 1.3446035385131836\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/16...\n",
            "2025-03-26 06:01:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/16...\n",
            "2025-03-26 06:01:21 INFO [auto_gptq.quantization.gptq] duration: 0.7716140747070312\n",
            "2025-03-26 06:01:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.17679110169410706\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/16...\n",
            "2025-03-26 06:01:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/16...\n",
            "2025-03-26 06:01:22 INFO [auto_gptq.quantization.gptq] duration: 1.020348310470581\n",
            "2025-03-26 06:01:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.4863243103027344\n",
            "INFO - Quantizing self_attn.o_proj in layer 12/16...\n",
            "2025-03-26 06:01:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/16...\n",
            "2025-03-26 06:01:23 INFO [auto_gptq.quantization.gptq] duration: 1.3155698776245117\n",
            "2025-03-26 06:01:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.016571924090385437\n",
            "INFO - Quantizing mlp.up_proj in layer 12/16...\n",
            "2025-03-26 06:01:23 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/16...\n",
            "2025-03-26 06:01:24 INFO [auto_gptq.quantization.gptq] duration: 1.0728082656860352\n",
            "2025-03-26 06:01:24 INFO [auto_gptq.quantization.gptq] avg loss: 2.2410202026367188\n",
            "INFO - Quantizing mlp.gate_proj in layer 12/16...\n",
            "2025-03-26 06:01:24 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/16...\n",
            "2025-03-26 06:01:25 INFO [auto_gptq.quantization.gptq] duration: 0.7945482730865479\n",
            "2025-03-26 06:01:25 INFO [auto_gptq.quantization.gptq] avg loss: 3.5003609657287598\n",
            "INFO - Quantizing mlp.down_proj in layer 12/16...\n",
            "2025-03-26 06:01:25 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/16...\n",
            "2025-03-26 06:01:30 INFO [auto_gptq.quantization.gptq] duration: 4.656956434249878\n",
            "2025-03-26 06:01:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.05190528556704521\n",
            "INFO - Start quantizing layer 13/16\n",
            "2025-03-26 06:01:30 INFO [auto_gptq.modeling._base] Start quantizing layer 13/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 13/16...\n",
            "2025-03-26 06:01:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/16...\n",
            "2025-03-26 06:01:31 INFO [auto_gptq.quantization.gptq] duration: 1.1413440704345703\n",
            "2025-03-26 06:01:31 INFO [auto_gptq.quantization.gptq] avg loss: 1.2833020687103271\n",
            "INFO - Quantizing self_attn.v_proj in layer 13/16...\n",
            "2025-03-26 06:01:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/16...\n",
            "2025-03-26 06:01:32 INFO [auto_gptq.quantization.gptq] duration: 0.7844841480255127\n",
            "2025-03-26 06:01:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.18909502029418945\n",
            "INFO - Quantizing self_attn.q_proj in layer 13/16...\n",
            "2025-03-26 06:01:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/16...\n",
            "2025-03-26 06:01:33 INFO [auto_gptq.quantization.gptq] duration: 0.7955613136291504\n",
            "2025-03-26 06:01:33 INFO [auto_gptq.quantization.gptq] avg loss: 2.4622111320495605\n",
            "INFO - Quantizing self_attn.o_proj in layer 13/16...\n",
            "2025-03-26 06:01:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/16...\n",
            "2025-03-26 06:01:34 INFO [auto_gptq.quantization.gptq] duration: 1.2930631637573242\n",
            "2025-03-26 06:01:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.017879461869597435\n",
            "INFO - Quantizing mlp.up_proj in layer 13/16...\n",
            "2025-03-26 06:01:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/16...\n",
            "2025-03-26 06:01:35 INFO [auto_gptq.quantization.gptq] duration: 1.3466603755950928\n",
            "2025-03-26 06:01:35 INFO [auto_gptq.quantization.gptq] avg loss: 2.347174644470215\n",
            "INFO - Quantizing mlp.gate_proj in layer 13/16...\n",
            "2025-03-26 06:01:35 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/16...\n",
            "2025-03-26 06:01:36 INFO [auto_gptq.quantization.gptq] duration: 0.8046567440032959\n",
            "2025-03-26 06:01:36 INFO [auto_gptq.quantization.gptq] avg loss: 3.5164809226989746\n",
            "INFO - Quantizing mlp.down_proj in layer 13/16...\n",
            "2025-03-26 06:01:36 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/16...\n",
            "2025-03-26 06:01:41 INFO [auto_gptq.quantization.gptq] duration: 4.6586222648620605\n",
            "2025-03-26 06:01:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.06384842097759247\n",
            "INFO - Start quantizing layer 14/16\n",
            "2025-03-26 06:01:41 INFO [auto_gptq.modeling._base] Start quantizing layer 14/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 14/16...\n",
            "2025-03-26 06:01:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/16...\n",
            "2025-03-26 06:01:42 INFO [auto_gptq.quantization.gptq] duration: 1.3073337078094482\n",
            "2025-03-26 06:01:42 INFO [auto_gptq.quantization.gptq] avg loss: 1.3293285369873047\n",
            "INFO - Quantizing self_attn.v_proj in layer 14/16...\n",
            "2025-03-26 06:01:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/16...\n",
            "2025-03-26 06:01:43 INFO [auto_gptq.quantization.gptq] duration: 0.7804994583129883\n",
            "2025-03-26 06:01:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.33777374029159546\n",
            "INFO - Quantizing self_attn.q_proj in layer 14/16...\n",
            "2025-03-26 06:01:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/16...\n",
            "2025-03-26 06:01:44 INFO [auto_gptq.quantization.gptq] duration: 0.7693629264831543\n",
            "2025-03-26 06:01:44 INFO [auto_gptq.quantization.gptq] avg loss: 2.8961353302001953\n",
            "INFO - Quantizing self_attn.o_proj in layer 14/16...\n",
            "2025-03-26 06:01:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/16...\n",
            "2025-03-26 06:01:45 INFO [auto_gptq.quantization.gptq] duration: 1.013118028640747\n",
            "2025-03-26 06:01:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.022582456469535828\n",
            "INFO - Quantizing mlp.up_proj in layer 14/16...\n",
            "2025-03-26 06:01:45 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/16...\n",
            "2025-03-26 06:01:46 INFO [auto_gptq.quantization.gptq] duration: 1.360546588897705\n",
            "2025-03-26 06:01:46 INFO [auto_gptq.quantization.gptq] avg loss: 2.711515426635742\n",
            "INFO - Quantizing mlp.gate_proj in layer 14/16...\n",
            "2025-03-26 06:01:46 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/16...\n",
            "2025-03-26 06:01:47 INFO [auto_gptq.quantization.gptq] duration: 1.0920977592468262\n",
            "2025-03-26 06:01:47 INFO [auto_gptq.quantization.gptq] avg loss: 3.769101619720459\n",
            "INFO - Quantizing mlp.down_proj in layer 14/16...\n",
            "2025-03-26 06:01:47 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/16...\n",
            "2025-03-26 06:01:52 INFO [auto_gptq.quantization.gptq] duration: 4.646093845367432\n",
            "2025-03-26 06:01:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.09424137324094772\n",
            "INFO - Start quantizing layer 15/16\n",
            "2025-03-26 06:01:52 INFO [auto_gptq.modeling._base] Start quantizing layer 15/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 15/16...\n",
            "2025-03-26 06:01:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/16...\n",
            "2025-03-26 06:01:53 INFO [auto_gptq.quantization.gptq] duration: 1.1561834812164307\n",
            "2025-03-26 06:01:53 INFO [auto_gptq.quantization.gptq] avg loss: 1.3613706827163696\n",
            "INFO - Quantizing self_attn.v_proj in layer 15/16...\n",
            "2025-03-26 06:01:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/16...\n",
            "2025-03-26 06:01:54 INFO [auto_gptq.quantization.gptq] duration: 0.7730910778045654\n",
            "2025-03-26 06:01:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.7147023677825928\n",
            "INFO - Quantizing self_attn.q_proj in layer 15/16...\n",
            "2025-03-26 06:01:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/16...\n",
            "2025-03-26 06:01:55 INFO [auto_gptq.quantization.gptq] duration: 0.7616941928863525\n",
            "2025-03-26 06:01:55 INFO [auto_gptq.quantization.gptq] avg loss: 2.7590270042419434\n",
            "INFO - Quantizing self_attn.o_proj in layer 15/16...\n",
            "2025-03-26 06:01:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/16...\n",
            "2025-03-26 06:01:56 INFO [auto_gptq.quantization.gptq] duration: 1.006422758102417\n",
            "2025-03-26 06:01:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.041947342455387115\n",
            "INFO - Quantizing mlp.up_proj in layer 15/16...\n",
            "2025-03-26 06:01:56 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/16...\n",
            "2025-03-26 06:01:57 INFO [auto_gptq.quantization.gptq] duration: 1.0719356536865234\n",
            "2025-03-26 06:01:57 INFO [auto_gptq.quantization.gptq] avg loss: 2.9586358070373535\n",
            "INFO - Quantizing mlp.gate_proj in layer 15/16...\n",
            "2025-03-26 06:01:57 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/16...\n",
            "2025-03-26 06:01:58 INFO [auto_gptq.quantization.gptq] duration: 1.047424077987671\n",
            "2025-03-26 06:01:58 INFO [auto_gptq.quantization.gptq] avg loss: 4.411075592041016\n",
            "INFO - Quantizing mlp.down_proj in layer 15/16...\n",
            "2025-03-26 06:01:58 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/16...\n",
            "2025-03-26 06:02:03 INFO [auto_gptq.quantization.gptq] duration: 4.703522443771362\n",
            "2025-03-26 06:02:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.14218729734420776\n",
            "INFO - Start quantizing layer 16/16\n",
            "2025-03-26 06:02:03 INFO [auto_gptq.modeling._base] Start quantizing layer 16/16\n",
            "INFO - Quantizing self_attn.k_proj in layer 16/16...\n",
            "2025-03-26 06:02:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/16...\n",
            "2025-03-26 06:02:04 INFO [auto_gptq.quantization.gptq] duration: 1.1199235916137695\n",
            "2025-03-26 06:02:04 INFO [auto_gptq.quantization.gptq] avg loss: 1.4571707248687744\n",
            "INFO - Quantizing self_attn.v_proj in layer 16/16...\n",
            "2025-03-26 06:02:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/16...\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.quantization.gptq] duration: 0.7903082370758057\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.7084294557571411\n",
            "INFO - Quantizing self_attn.q_proj in layer 16/16...\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/16...\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.quantization.gptq] duration: 0.7895801067352295\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.quantization.gptq] avg loss: 2.495234727859497\n",
            "INFO - Quantizing self_attn.o_proj in layer 16/16...\n",
            "2025-03-26 06:02:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/16...\n",
            "2025-03-26 06:02:06 INFO [auto_gptq.quantization.gptq] duration: 1.018298625946045\n",
            "2025-03-26 06:02:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.233761727809906\n",
            "INFO - Quantizing mlp.up_proj in layer 16/16...\n",
            "2025-03-26 06:02:06 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/16...\n",
            "2025-03-26 06:02:08 INFO [auto_gptq.quantization.gptq] duration: 1.0769765377044678\n",
            "2025-03-26 06:02:08 INFO [auto_gptq.quantization.gptq] avg loss: 3.604949951171875\n",
            "INFO - Quantizing mlp.gate_proj in layer 16/16...\n",
            "2025-03-26 06:02:08 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/16...\n",
            "2025-03-26 06:02:09 INFO [auto_gptq.quantization.gptq] duration: 1.295226812362671\n",
            "2025-03-26 06:02:09 INFO [auto_gptq.quantization.gptq] avg loss: 4.789373397827148\n",
            "INFO - Quantizing mlp.down_proj in layer 16/16...\n",
            "2025-03-26 06:02:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/16...\n",
            "2025-03-26 06:02:14 INFO [auto_gptq.quantization.gptq] duration: 4.885014533996582\n",
            "2025-03-26 06:02:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.40205711126327515\n",
            "2025-03-26 06:02:14 INFO [auto_gptq.modeling._utils] Packing model...\n",
            "2025-03-26 06:02:14 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
            "2025-03-26 06:02:15 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
            "2025-03-26 06:02:15 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
            "2025-03-26 06:02:15 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
            "2025-03-26 06:02:15 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
            "2025-03-26 06:02:16 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
            "2025-03-26 06:02:16 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
            "2025-03-26 06:02:17 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
            "2025-03-26 06:02:18 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
            "2025-03-26 06:02:18 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
            "2025-03-26 06:02:18 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
            "2025-03-26 06:02:18 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
            "2025-03-26 06:02:19 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
            "2025-03-26 06:02:19 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
            "2025-03-26 06:02:20 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
            "2025-03-26 06:02:21 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
            "2025-03-26 06:02:21 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
            "2025-03-26 06:02:21 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
            "2025-03-26 06:02:21 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
            "2025-03-26 06:02:22 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
            "2025-03-26 06:02:23 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
            "2025-03-26 06:02:24 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
            "2025-03-26 06:02:24 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
            "2025-03-26 06:02:24 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
            "2025-03-26 06:02:25 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
            "2025-03-26 06:02:25 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
            "2025-03-26 06:02:25 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
            "2025-03-26 06:02:26 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
            "2025-03-26 06:02:27 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
            "2025-03-26 06:02:27 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
            "2025-03-26 06:02:28 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
            "2025-03-26 06:02:28 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
            "2025-03-26 06:02:28 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
            "2025-03-26 06:02:29 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
            "2025-03-26 06:02:29 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
            "2025-03-26 06:02:30 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
            "2025-03-26 06:02:31 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
            "2025-03-26 06:02:31 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
            "2025-03-26 06:02:31 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
            "2025-03-26 06:02:31 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
            "2025-03-26 06:02:32 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
            "2025-03-26 06:02:32 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
            "2025-03-26 06:02:34 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
            "2025-03-26 06:02:34 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
            "2025-03-26 06:02:34 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
            "2025-03-26 06:02:34 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
            "2025-03-26 06:02:34 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
            "2025-03-26 06:02:35 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
            "2025-03-26 06:02:36 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
            "2025-03-26 06:02:37 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
            "2025-03-26 06:02:38 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
            "2025-03-26 06:02:38 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
            "2025-03-26 06:02:38 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
            "2025-03-26 06:02:38 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
            "2025-03-26 06:02:39 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
            "2025-03-26 06:02:39 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
            "2025-03-26 06:02:40 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
            "2025-03-26 06:02:41 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
            "2025-03-26 06:02:41 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
            "2025-03-26 06:02:41 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
            "2025-03-26 06:02:41 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
            "2025-03-26 06:02:42 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
            "2025-03-26 06:02:43 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
            "2025-03-26 06:02:44 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
            "2025-03-26 06:02:44 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
            "2025-03-26 06:02:44 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
            "2025-03-26 06:02:44 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
            "2025-03-26 06:02:44 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
            "2025-03-26 06:02:45 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
            "2025-03-26 06:02:46 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
            "2025-03-26 06:02:48 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
            "2025-03-26 06:02:48 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
            "2025-03-26 06:02:48 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
            "2025-03-26 06:02:48 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
            "2025-03-26 06:02:49 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
            "2025-03-26 06:02:50 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
            "2025-03-26 06:02:51 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
            "2025-03-26 06:02:52 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
            "2025-03-26 06:02:52 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
            "2025-03-26 06:02:52 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
            "2025-03-26 06:02:53 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
            "2025-03-26 06:02:53 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
            "2025-03-26 06:02:54 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
            "2025-03-26 06:02:55 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
            "2025-03-26 06:02:57 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
            "2025-03-26 06:02:57 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
            "2025-03-26 06:02:57 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
            "2025-03-26 06:02:57 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
            "2025-03-26 06:02:57 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
            "2025-03-26 06:02:58 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
            "2025-03-26 06:02:59 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
            "2025-03-26 06:03:00 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
            "2025-03-26 06:03:01 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
            "2025-03-26 06:03:01 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
            "2025-03-26 06:03:01 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
            "2025-03-26 06:03:01 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
            "2025-03-26 06:03:02 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
            "2025-03-26 06:03:03 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
            "2025-03-26 06:03:04 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
            "2025-03-26 06:03:04 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
            "2025-03-26 06:03:04 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
            "2025-03-26 06:03:04 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
            "2025-03-26 06:03:04 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
            "2025-03-26 06:03:05 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
            "2025-03-26 06:03:06 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
            "2025-03-26 06:03:07 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
            "2025-03-26 06:03:07 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
            "2025-03-26 06:03:08 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
            "2025-03-26 06:03:08 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
            "2025-03-26 06:03:08 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
            "2025-03-26 06:03:09 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
            "2025-03-26 06:03:09 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
            "2025-03-26 06:03:10 INFO [auto_gptq.modeling._utils] Model packed.\n",
            "quantization took:  233.9286s\n",
            "Device set to use cuda:0\n",
            "The model 'LlamaGPTQForCausalLM' is not supported for . Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForCausalLM', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
            "prompt: Instruction:\n",
            "Create a travel itinerary for a 5-day trip to Las Vegas\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Day 1: Arrive in Las Vegas. Explore the city. \n",
            "Day 2: Visit the Grand Canyon. \n",
            "Day 3: Visit the Valley of Fire or Red Rock Canyon. \n",
            "Day 4: Visit some of the casinos and take a show at one of the resorts. \n",
            "Day 5: Visit Old Las Vegas, the Neon Museum, and Freemont Street.\n",
            "------------------------------------------\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "quant:   1. 5-day travel itinerary for a 5-day trip to Las Vegas\n",
            "  2. The first 3 days will be in Las Vegas\n",
            "  3. The last 2 days will be in Las Vegas and the Grand Canyon\n",
            "  4. The itinerary should be in the format of a schedule, with each day listed as a day and a time, such as \"9:00 AM - 12:00 PM\"\n",
            "  5. The itinerary should be in the format of a schedule, with each day listed as a day and a time, such as \"9:00 AM - 12:00 PM\"\n",
            "  6. The itinerary should be in the format of a schedule, with each day listed as a day and a time, such as \"9:00 AM - 12:00 PM\"\n",
            "  7. The itinerary should be in the format of a schedule, with each day listed as a day and a time, such as \"9:00 AM\n",
            "generate 207 tokens using  26.8041s, 7.722704074607528 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Tell me about a famous writer.\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: William Shakespeare is one of the most famous writers in history. He is known for his plays, poems, and sonnets and is widely regarded as the greatest writer in the English language. His plays have been adapted for stage and screen and his works are studied in classrooms around the world.\n",
            "------------------------------------------\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "quant: \"Dear Professor, I have been asked to tell you about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer. I have given you a lot of information about a famous writer. I have done my best to tell you about a famous writer.\n",
            "generate 186 tokens using  23.4824s, 7.920821728585588 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Program a search engine to find a person based on given parameters.\n",
            "Input:\n",
            "Parameters: Name-John Smith, Height-5'10\", Age-30\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Pseudo-code:\n",
            "\n",
            "1. Search the database for names matching \"John Smith\".\n",
            "2. Filter out records that have a height of 5'10\".\n",
            "3. Further narrow down results to records that have an age of 30.\n",
            "4. Return the records found or display a \"Not Found\" message.\n",
            "------------------------------------------\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "quant: Person found: John Smith, 30 years old.\n",
            "\n",
            "generate 12 tokens using  1.4999s, 8.00045397473478 tokens/s.\n",
            "==========================================\n",
            "prompt: Instruction:\n",
            "Create a type definition for the word 'book'.\n",
            "Output:\n",
            "\n",
            "------------------------------------------\n",
            "golden: Book: A book is a collection of information, stories, ideas, or the like that is produced in the form of writing, images, or some other medium and is issued to the public or otherwise available for public use. It is usually printed or in digital form. Its contents are typically organized in an orderly fashion, such as in chapters, articles, or stories.\n",
            "------------------------------------------\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "quant: This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is a book.\n",
            "This is\n",
            "generate 203 tokens using  25.1437s, 8.073598842862541 tokens/s.\n",
            "==========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --pretrained_model_dir \"meta-llama/Llama-3.2-1B\" -trust_remote_code --max_new_tokens 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQVe6mx9Dm-Z",
        "outputId": "10765cf3-05d7-4af7-9bc1-d8a2f4afe92e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:06:36.498738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969196.527081   21469 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969196.535761   21469 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:06:36.564925: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "usage: generation_speed.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                           [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH] [--from_pretrained]\n",
            "                           [--model_basename MODEL_BASENAME]\n",
            "                           [--quantize_config_save_dir QUANTIZE_CONFIG_SAVE_DIR]\n",
            "                           [--trust_remote_code] [--use_triton] [--use_safetensors]\n",
            "                           [--use_fast_tokenizer] [--disable_exllama]\n",
            "                           [--no_inject_fused_attention] [--no_inject_fused_mlp]\n",
            "                           [--num_samples NUM_SAMPLES] [--per_gpu_max_memory PER_GPU_MAX_MEMORY]\n",
            "                           [--cpu_max_memory CPU_MAX_MEMORY] [--max_new_tokens MAX_NEW_TOKENS]\n",
            "                           [--do_sample] [--num_beams NUM_BEAMS]\n",
            "generation_speed.py: error: unrecognized arguments: --pretrained_model_dir meta-llama/Llama-3.2-1B --quant_batch_size 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --pretrained_model_dir \"meta-llama/Llama-3.2-1B\" --trust_remote_code --max_new_tokens 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE0eNU1-ECtR",
        "outputId": "c7f3feb1-235d-42d7-ba6a-9f9e7614c22b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:09:41.746138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969381.767486   22257 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969381.773888   22257 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:09:41.794871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "usage: generation_speed.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
            "                           [--tokenizer_name_or_path TOKENIZER_NAME_OR_PATH] [--from_pretrained]\n",
            "                           [--model_basename MODEL_BASENAME]\n",
            "                           [--quantize_config_save_dir QUANTIZE_CONFIG_SAVE_DIR]\n",
            "                           [--trust_remote_code] [--use_triton] [--use_safetensors]\n",
            "                           [--use_fast_tokenizer] [--disable_exllama]\n",
            "                           [--no_inject_fused_attention] [--no_inject_fused_mlp]\n",
            "                           [--num_samples NUM_SAMPLES] [--per_gpu_max_memory PER_GPU_MAX_MEMORY]\n",
            "                           [--cpu_max_memory CPU_MAX_MEMORY] [--max_new_tokens MAX_NEW_TOKENS]\n",
            "                           [--do_sample] [--num_beams NUM_BEAMS]\n",
            "generation_speed.py: error: unrecognized arguments: --pretrained_model_dir meta-llama/Llama-3.2-1B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path \"meta-llama/Llama-3.2-1B\" --trust_remote_code --max_new_tokens 11 --num_samples 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrMOnwGUEDAR",
        "outputId": "5f7550f5-90f6-4882-84a7-036fe93ffa25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:13:24.343380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969604.363398   23241 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969604.369599   23241 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:13:24.391043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "2025-03-26 06:13:27 INFO [__main__] max_memory: None\n",
            "2025-03-26 06:13:27 INFO [__main__] loading model and tokenizer\n",
            "WARNING - Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:13:28 WARNING [auto_gptq.modeling._base] Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:13:28 WARNING [auto_gptq.modeling._base] Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "2025-03-26 06:13:28 WARNING [auto_gptq.modeling._base] CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 326, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 272, in main\n",
            "    model, tokenizer = load_model_tokenizer(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 170, in load_model_tokenizer\n",
            "    model = AutoGPTQForCausalLM.from_quantized(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/auto.py\", line 135, in from_quantized\n",
            "    return quant_func(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 917, in from_quantized\n",
            "    quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path, **cached_file_kwargs, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 160, in from_pretrained\n",
            "    args_from_json = args_from_json[\"quantization_config\"]\n",
            "                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyError: 'quantization_config'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path \"meta-llama/Llama-3-2-1B\" --trust_remote_code --max_new_tokens 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqg4gO4kFe2C",
        "outputId": "461ce906-cc85-4d53-8170-2a32347e58a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:14:49.324783: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969689.357748   23622 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969689.368210   23622 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:14:49.400940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "2025-03-26 06:14:52 INFO [__main__] max_memory: None\n",
            "2025-03-26 06:14:52 INFO [__main__] loading model and tokenizer\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3-2-1B/resolve/main/tokenizer_config.json\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 424, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n",
            "    return _hf_hub_download_to_cache_dir(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 969, in _hf_hub_download_to_cache_dir\n",
            "    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1486, in _raise_on_head_call_error\n",
            "    raise head_call_error\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1376, in _get_metadata_or_catch_error\n",
            "    metadata = get_hf_file_metadata(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 1296, in get_hf_file_metadata\n",
            "    r = _request_wrapper(\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 280, in _request_wrapper\n",
            "    response = _request_wrapper(\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\", line 304, in _request_wrapper\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 458, in hf_raise_for_status\n",
            "    raise _format(RepositoryNotFoundError, message, response) from e\n",
            "huggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-67e39b5c-39bbda3f50805d5f36a7e4a0;897aaa68-fef0-4778-8f45-fb7391bb129f)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/meta-llama/Llama-3-2-1B/resolve/main/tokenizer_config.json.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 326, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 272, in main\n",
            "    model, tokenizer = load_model_tokenizer(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 154, in load_model_tokenizer\n",
            "    tokenizer = AutoTokenizer.from_pretrained(\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 910, in from_pretrained\n",
            "    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 742, in get_tokenizer_config\n",
            "    resolved_config_file = cached_file(\n",
            "                           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 266, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\", line 456, in cached_files\n",
            "    raise EnvironmentError(\n",
            "OSError: meta-llama/Llama-3-2-1B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --tokenXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgDt8JQbFo0a",
        "outputId": "97808108-4981-4950-8e64-f821b15929d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwLmlcsDFpHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path \"Qwen/Qwen2.5-1.5B\" --trust_remote_code --max_new_tokens 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJpf6wqoF2sS",
        "outputId": "56194014-b30e-4918-9bbc-7f17db0e3775"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:15:45.758339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969745.795435   23856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969745.805831   23856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:15:45.839234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "2025-03-26 06:15:49 INFO [__main__] max_memory: None\n",
            "2025-03-26 06:15:49 INFO [__main__] loading model and tokenizer\n",
            "WARNING - Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:15:50 WARNING [auto_gptq.modeling._base] Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:15:50 WARNING [auto_gptq.modeling._base] Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "2025-03-26 06:15:50 WARNING [auto_gptq.modeling._base] CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 326, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 272, in main\n",
            "    model, tokenizer = load_model_tokenizer(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 170, in load_model_tokenizer\n",
            "    model = AutoGPTQForCausalLM.from_quantized(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/auto.py\", line 135, in from_quantized\n",
            "    return quant_func(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 917, in from_quantized\n",
            "    quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path, **cached_file_kwargs, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 160, in from_pretrained\n",
            "    args_from_json = args_from_json[\"quantization_config\"]\n",
            "                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyError: 'quantization_config'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/AutoGPTQ/examples/benchmark\n",
        "!python /content/AutoGPTQ/examples/benchmark/generation_speed.py --model_name_or_path \"openai-community/gpt2\" --trust_remote_code --max_new_tokens 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J04uBCI-F48K",
        "outputId": "e0a2bc6b-dc9f-4ab8-d61b-0c51f011f1aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AutoGPTQ/examples/benchmark\n",
            "2025-03-26 06:16:47.221853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742969807.257140   24120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742969807.268076   24120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-26 06:16:47.288760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "CUDA extension not installed.\n",
            "CUDA extension not installed.\n",
            "2025-03-26 06:16:50 INFO [__main__] max_memory: None\n",
            "2025-03-26 06:16:50 INFO [__main__] loading model and tokenizer\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 169kB/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 5.88MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 7.07MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 6.46MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 7.96MB/s]\n",
            "WARNING - Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:16:52 WARNING [auto_gptq.modeling._base] Exllama kernel is not installed, reset disable_exllama to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "2025-03-26 06:16:52 WARNING [auto_gptq.modeling._base] Exllamav2 kernel is not installed, reset disable_exllamav2 to True. This may because you installed auto_gptq using a pre-build wheel on Windows, in which exllama_kernels are not compiled. To use exllama_kernels to further speedup inference, you can re-install auto_gptq from source.\n",
            "WARNING - CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "2025-03-26 06:16:52 WARNING [auto_gptq.modeling._base] CUDA kernels for auto_gptq are not installed, this will result in very slow inference speed. This may because:\n",
            "1. You disabled CUDA extensions compilation by setting BUILD_CUDA_EXT=0 when install auto_gptq from source.\n",
            "2. You are using pytorch without CUDA support.\n",
            "3. CUDA and nvcc are not installed in your device.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 326, in <module>\n",
            "    main()\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 272, in main\n",
            "    model, tokenizer = load_model_tokenizer(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/AutoGPTQ/examples/benchmark/generation_speed.py\", line 170, in load_model_tokenizer\n",
            "    model = AutoGPTQForCausalLM.from_quantized(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/auto.py\", line 135, in from_quantized\n",
            "    return quant_func(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 917, in from_quantized\n",
            "    quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path, **cached_file_kwargs, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/auto_gptq/modeling/_base.py\", line 160, in from_pretrained\n",
            "    args_from_json = args_from_json[\"quantization_config\"]\n",
            "                     ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyError: 'quantization_config'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nN-luQ7OGH56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}